# 密集层（Dense Layer）、全连接层（Fully Connected Layer）或线性层（Linear Layer）
这几种都是同一种神经网络，最基础最简单的形式
公式：$\text{输出}=\text{输入} \times \text{权重}+\text{偏置}$
即$Y=X\times W^T+b$
```python
import torch
import torch.nn as nn
# 定义线性层
linear_layer = nn.Linear(in_features=10, out_features=5)
# 使用state_dict()方法查看参数
params = linear_layer.state_dict()
# 创建输入
input_data = torch.randn(1, 10)  # 假设输入维度为10
# 应用线性变换
print(input_data.size())
print(input_data)
print("--------------------")
output = linear_layer(input_data)
print(output.size())
print(output)
print("--------------------")
# 打印参数
print(params['weight'].size()
print(params['bias'].size())
print("模块的参数：", params)
```
```bash
torch.Size([1, 10]) 
tensor([[-1.4128, -1.1393, -1.0336, -0.7670, -0.1509, 0.5986, 0.7047, 0.1613, -0.4129, -0.8614]]) 
-------------------- 
torch.Size([1, 5]) 
tensor([[ 0.3988, -0.3576, 0.8914, -0.0833, -1.0327]], 
        grad_fn=<AddmmBackward0>) 
-------------------- 
torch.Size([5, 10]) 
torch.Size([5]) 
模块的参数： OrderedDict([('weight', tensor([[-0.0596, 0.0869, -0.2546, 0.1188, 0.1906, 0.2167, 0.0602, -0.1506, -0.2983, 0.1872], 
    [-0.0805, 0.2903, -0.1096, 0.0823, -0.1069, -0.1137, -0.1887, 0.2933, 0.2349, 0.1221], 
    [-0.2290, 0.1153, -0.3059, -0.0953, -0.1931, 0.1932, 0.1197, 0.2735, -0.0698, 0.1419], 
    [-0.0936, 0.0988, 0.2356, -0.0759, 0.1534, 0.2863, -0.0685, 0.2739, 0.2967, 0.2491], 
    [ 0.2887, 0.1022, -0.0823, 0.2715, -0.0111, 0.1162, -0.2033, -0.3062, 0.0927, -0.0621]])), ('bias', tensor([ 0.1607, 0.1490, 0.1301, 0.2752, -0.2789]))])
```