### **梯度下降算法的核心思想**
通过迭代调整模型参数 $w$，使得损失函数$E(w)$ 最小化。每一步的更新方向是当前参数点处梯度的负方向，步长由学习率 $γ$ 控制。
### **详细步骤**
1. **初始化参数** 随机或指定初始参数值 w(0)，例如 w(0)=0 或随机初始化。
2. **设置超参数**
    - **学习率（Learning Rate）**$γ$：控制更新步长（典型值如 0.01、0.001）。
    - **最大迭代次数**：防止无限循环。
    - **收敛阈值**：当参数变化或损失变化小于阈值时停止迭代。
3. **迭代更新参数** 对于第 $t$ 次迭代（$t=0,1,2,…$）：
    - **计算梯度**：计算损失函数在当前参数处的梯度$g^{(t)}=\frac{\partial E}{\partial w}|_{W=w^{(t)}}$梯度表示损失函数在 $w(t)$ 处的上升方向，负梯度即下降方向。
    - **更新参数**：沿负梯度方向调整参数 $w(t+1)=w(t)−γ⋅g(t)$
    - **检查收敛条件**：
        - 参数变化足够小：$∥w(t+1)−w(t)∥<ϵ$
        - 损失变化足够小：$∣E(w(t+1))−E(w(t))∣<ϵ$
        - 达到最大迭代次数。
4. **终止迭代** 当满足任一收敛条件时，输出最终参数 $w^∗$ 。
### **关键细节与变种**
1. **学习率的选择**
    - 过大：可能导致振荡甚至发散。
    - 过小：收敛速度慢，易陷入局部极小值。
    - 自适应学习率方法（如Adam、RMSProp）可动态调整学习率。
2. **梯度计算方式**
    - **批量梯度下降（BGD）**：使用全部数据计算梯度，稳定但计算成本高。
    - **随机梯度下降（SGD）**：每次随机选一个样本计算梯度，速度快但噪声大。
    - **小批量梯度下降（Mini-batch GD）**：折中方案，每次用一小批数据（如32、64个样本）。
3. **损失函数的性质**
    - 凸函数：梯度下降能收敛到全局最优。
    - 非凸函数：可能收敛到局部最优或鞍点，需结合动量（Momentum）等方法优化。

### **示例说明（简单二次函数）**
以一元二次函数 $f(x)=(x−1)2+1$ 为例：
1. 初始点 $x0=6$，计算梯度 $f′(x)=2x−2$。
2. 学习率 $η=0.05$，更新过程为：$x1=6−0.05×10=5.5,x2=5.5−0.05×9=5.05,…$ 经过多次迭代后逼近最小值点 $x=1$。