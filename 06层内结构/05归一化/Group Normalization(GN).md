对比**Batch Normalization**的优势：

- 当批量大小过小时，统计量不够稳定，导致训练效果下降。

  GN 不依赖于 batch 维度，而是基于通道分组进行归一化，因此在小批量甚至 batch size 为 1 的情况下依然有效。

- GN 通过对特征图的分组归一化，能够减少通道之间的协方差偏移（Covariance Shift），增强特征表达的稳定性，从而提高模型的泛化能力。

## 实现方法

GN 的核心思想是将特征图的通道分成若干组，对每组内的特征进行归一化处理。具体步骤如下：

1. **输入维度**
    假设输入特征图为一个 4D 张量，形状为 **(N, C, H, W)**，其中：

   - `N` 是 batch size
   - `C` 是通道数
   - `H` 和 `W` 是特征图的高和宽

2. **分组**
    将通道 `C` 分成 `G` 组，每组包含 `C/G` 个通道（假设 `C` 能被 `G` 整除）。每一组中的通道共享归一化的均值和方差。

3. **计算组内统计量**
    对每一组的所有通道和空间位置计算均值和方差：

   - 均值:$\mu = \frac{1}{m} \sum_{i=1}^{m} x_i$

   - 方差:$\sigma^2=\frac{1}{m}\sum_{i=1}^m(x_i-\mu)^2$

     其中，m 是每组的元素总数：$m = \frac{C}{G} \cdot H \cdot W$

4. **归一化**
    对每个元素进行归一化：$\hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}$

   其中，`ε` 是一个小的平滑项，防止分母为 0。

5. **学习可训练参数**
    每个特征通道都引入可学习的尺度参数 `γ` 和偏移量 `β`，使得输出为：$y_i = \gamma \hat{x}_i + \beta$

## **Group Normalization 的注意事项**

1. **组数的选择（G 的设置）**
   - 组数 `G` 是 GN 的核心超参数，通常设置为 2、4、8、32 等值。
   - 如果组数过大（接近于通道数 `C`），GN 会退化为 Layer Normalization（LN）；如果组数为 1，则退化为 Instance Normalization（IN）。
   - 经验上，`G=32` 在许多任务中表现较好，但需要根据具体任务和模型调整。
2. **通道数必须能被组数整除**
    通道数 `C` 必须能被组数 `G` 整除，否则无法对通道进行分组。
3. **适用场景**
    GN 更适合小批量或动态输入的场景，例如：
   - 小型数据集或内存受限导致的 batch size 小
   - 目标检测、语义分割等任务
   - RNN 或 Transformers 等架构
4. **与其他归一化方法的差异**
   - **Batch Normalization（BN）**：依赖 batch 维度的统计量，在小 batch size 下效果较差。
   - **Layer Normalization（LN）**：对每个样本的所有通道进行归一化，适合 NLP 和 RNN。
   - **Instance Normalization（IN）**：对每个样本的每个通道单独归一化，常用于图像生成任务（如风格迁移）。
   - **Group Normalization（GN）**：介于 LN 和 IN 之间，对通道进行分组归一化，适用于小批量训练。
5. **计算成本**
    GN 的计算复杂度与 LN 和 BN 相当，但由于不依赖 batch 统计量，训练时的分布更稳定，易于并行。