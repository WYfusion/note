策略梯度法（Policy Gradient）是强化学习中的一类核心算法，其核心思想是**直接优化策略函数**，通过参数化策略并沿梯度方向调整参数，以最大化长期累积奖励。与基于值函数的方法（如Q-learning）不同，策略梯度法不需要先估计动作价值函数，而是直接学习最优策略。以下从多个维度详细解析其原理与特性：
# 基本前提

$s$作为输入，$\boldsymbol{a}$是实际输出，$\widehat{\boldsymbol{a}}$是期望有***益***输出，$\widehat{a}^{\prime}$是期望有***害***输出。
1. 对于期望有***益***输出，根据实际输出与期望有***益***输出计算损失函数为$e_1$
2. 对于期望有***害***输出，根据实际输出与期望有***害***输出计算损失函数为$e_2$
使用$e_1-e_2$来相减即可实现综合损失函数。
![[Pasted image 20250320154948.png|600]]
举例下面是两个期望输出的例子，分别用**1**与**2**所标识，**2**在**1**的基础上添加了有益有害的程度。
![[Pasted image 20250320160629.png|600]]
于是当下的难点就在于如何定义$\{s_i,\widehat{\boldsymbol{a}_i}\}$对，如何确定$A_i$权重。
## 期望输出的确定
下图给出了一个版本0：
若输入为$s_1$输出为$a_1$得到的reward是正的，则下次可以执行$a_1$
若输入为$s_2$输出为$a_2$得到的reward是负的，则下次不要执行$a_2$

> [!NOTE]
> 这并不是一个好的版本，因为这个版本只可以短视的注意一个点上的好与坏，可能出现$a_1$对后续的负面影响很大，而$a_2$对后续的正面影响很大的情况。例如未雨绸缪的弃车帅。因此这种方案无法做到整体上的最优。

在实际的应用中也会常常出现牺牲当下的利益以实现最终总体上的最优。

![[Pasted image 20250320162319.png|600]]
### 版本1
通过累加当下及其之后的所有的reward数值作为当下的$A_i$权重。
- 好处是实现了当下虽然没有返回比较好的reward，但是可以看到未来有没有因为当下的行为$a_i$所造成的比较好的、比较坏的影响。例如下棋多看几步。
- 缺点在于当游戏执行的很长的时候，$a_1$很可能对于$r_N$的影响就很小了，得到后面比较好、坏的原因可能出自于$a_2$到$a_N$之间的行为所造成的，当然计算量也会很大。
![[Pasted image 20250320163827.png|600]]
### 版本2
针对于上面的版本1，对$A_i$引入衰减因子$\gamma$，使得不同时间距离的reward所造成的影响权重不同。
- 好处是解决了不同位置observation的reward归因于当前行为的程度
![[Pasted image 20250320164931.png|600]]
### 版本3
针对于上面两个版本，可能得到的分数都是正的(因为后面可能会有一个比较大的正向影响)，但实际上总会有某一个位置的行为是对后续的影响有坏处，因此需要使得这里的$A_i$变得有正有负，这样才可以体现具体某一个位置的有效影响。

> [!important]
> 问题在于这个baseline b是如何确定的？

![[Pasted image 20250320170546.png|600]]

# 一、核心思想与定义

1. 随机初始化网络的参数为$\theta^{0}$
2. 对于训练迭代 i=1 到 T
    - 使用actor参数$\theta^{i-1}$去和环境做互动
    - 获得数据对$\{s_i,\widehat{\boldsymbol{a}_i}\}$
    - 计算$A_1,A_2,...,A_N$
    - 计算损失函数值$L=\sum\limits_{n=1}^{N}\mathrm{А}_ne_n$
    - 常规梯度下降$\theta^i\leftarrow\theta^{i-1}-\eta\nabla L$，使得损失最小化。
    
    #On-policy-learning  
由于每次迭代后的参数不一定适合使用前面学到的(可能不那么出色的行为)所需一般需要获取当前的参数下智慧体同环境互动的行为以及各个参数。
同常规的深度学习不同，强化学习中的Policy Gradient必须在每一次训练的执行中获取数据对，并且只能做一次参数的迭代，迭代完毕后需要再次获取数据对。因此训练的是很慢的。

#Off-policy-learning  
也有一些办法可以实现使用$\theta^{i-1}$的行为来训练$\theta^{i}$，进而得到$\theta^{i+1}$