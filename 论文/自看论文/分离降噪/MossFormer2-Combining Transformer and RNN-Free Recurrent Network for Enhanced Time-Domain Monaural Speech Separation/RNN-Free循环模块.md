没有RNN的循环模块，Mossformer2的关键模块

---
# 瓶颈结构
1. **输入信号**为$X_{in}\in\mathbb{R}^{B \times N \times S}$，这里的 $B$ 是独立的音频序列数， $N$ 是每个时间步的特征向量长度（原嵌入维度），$S$ 是每个序列包含 $S$ 个时间步（如音频的采样点数）。
2. 使用[[残差结构（Residual Block）#^37c3dd|1×1卷积进行降维处理]]，卷积层尺寸 $N' \times N \times 1$（输出通道数 × 输入通道数 × 核大小），得到经过瓶颈层降维后的序列$X\in\mathbb{R}^{B \times N^{\prime}\times S}$，这里的 $N^{\prime}$ 是降维压缩后的模型嵌入维度。
## 一、GCU 的核心设计目标
GCU（门控卷积单元）是 MossFormer2 递归模块的核心组件，旨在**通过门控机制和卷积操作，高效捕捉语音序列的精细尺度递归模式**，同时避免传统 RNN 的顺序计算瓶颈。其设计融合了以下关键思想：
1. **门控机制**：受 GLU（Gated Linear Unit）启发，通过元素级乘法选择性融合信息，增强模型对关键特征的关注。
2. **扩张卷积**：通过扩张 FSMN 块扩大感受野，捕捉长距离时序依赖。
3. **密集连接**：在扩张 FSMN 内部引入密集连接，提升信息流动效率和梯度传播能力。
4. **卷积替代线性单元**：使用 1D 卷积替代传统线性变换，适配语音信号的局部相关性。
## 二、GCU 的完整架构


![[Pasted image 20250514162508.png|1000]]
GCU的输入是经过瓶颈结构后的$X\in\mathbb{R}^{B \times N^{\prime}\times S}$，而不是主线路中的$X_{in}\in\mathbb{R}^{B \times N \times S}$。注意Mossformer模块的输入和输出都是$\mathbb{R}^{B \times S\times N}$,也就是说需要频繁的对维度进行变形
$$\begin{aligned}U&=Conv\_{U(X)}\in\mathbb{R}^{B \times N^{\prime}\times S},\quad V=Conv\_{U(X)}\in\mathbb{R}^{B \times N^{\prime}\times S}&(1)\\Y&=Dilated\_{FSMN(V)}\in\mathbb{R}^{B \times N^{\prime}\times S}&(2)\\O&=X+(U\otimes Y)&(3)\end{aligned}$$
### 双分支卷积单元（Conv-U，图3中的D）
- **结构**：两个并行的 Conv-U 块，分别生成门控信号 U 和记忆信号 V。
- **Conv-U 块细节**
    - **LayerNorm**：对序列维度归一化，稳定训练。
    - **线性层 + SiLU 激活**：$\text{SiLU}(Wx + b)$，增强非线性表达。
    - **1D 深度卷积（D-Conv）**：核大小 $K=17$代码中的。
    - **残差连接**：$\text{Conv-U}(X) = X + \text{D-Conv}(\text{SiLU}(\text{LayerNorm}(X)))$。

- **输出**:  $U,V\in\mathbb{R}^{B \times N^{\prime}\times S}$（公式1）
### 扩张 FSMN 块（Dilated FSMN，图 3-B）
对应的是`models/mossformer2_se/fsmn.py`代码中的`UniDeepFsmn_dilated类`
- **目标**：通过多层扩张卷积模拟递归模式，替代传统 FSMN 的线性记忆块。
- **结构**：
    - **FFN 层**：$1×1$ 卷积保持维度，接 PReLU 激活，增强非线性。
    - **记忆层**：堆叠 $L$ 个 2D 扩张卷积块，每个块包含：
        1. **扩张卷积**：扩张率 $d=1, 2, ..., 2^{L-1}$，核大小 $3 \times 3$，分组卷积降低计算量。
            - $d=1$ 的扩张卷积本质是**标准局部卷积**，其数学作用是通过连续采样提取相邻时间步的短期特征。它是 GCU 多尺度特征融合的 “基石”，既保证了局部细节的保留，又为后续高扩张率层提供了高效的基础输入。这种设计在语音分离任务中至关重要，因为语音信号的局部连续性（如音素内的帧间关联）是分离不同说话人信号的关键线索。
        2. **实例归一化（InstanceNorm2d）**：对每个通道独立归一化，适配语音的局部统计特性。
        3. **PReLU 激活**：引入非线性。
        4. **密集连接**：第 $l$ 块输入为前 $l$ 块输出的拼接（图 3-B 右），公式： $X_l = \text{PReLU}(\text{InstanceNorm2d}(\text{Conv2d}([X_0, X_1, ..., X_{l-1}]))) + [X_0, X_1, ..., X_{l-1}]$
- **输出**：$Y \in \mathbb{R}^{N' \times S}$（公式 2），融合多尺度上下文的记忆信号。
注意这个FSMN是对于特征维度进行分组卷积。

| **扩张率 d** | **数学采样方式**                           | **感受野（核大小 \(K=3\)）**            | **适用场景** | **在 GCU 中的角色**     |
| --------- | ------------------------------------ | ------------------------------- | -------- | ------------------ |
| \(d=1\)   | 连续采样（无间隔）                            | 3（覆盖 \(t-1, t, t+1\)）           | 提取局部短期依赖 | 基础特征提取层，为高扩张率层提供输入 |
| \(d=2\)   | 间隔 1 个位置采样（\(x[t-2], x[t], x[t+2]\)） | 5（覆盖 \(t-2, t-1, t, t+1, t+2\)） | 捕捉中等距离依赖 | 中层特征融合，扩大感受野       |
| \(d=4\)   | 间隔 3 个位置采样（\(x[t-4], x[t], x[t+4]\)） | 9（覆盖 \(t-4, ..., t+4\)）         | 捕捉更长距离依赖 | 高层特征融合，覆盖全局上下文     |

### 门控融合（公式 3）
- **门控机制**：通过元素级乘法 $U \otimes Y$ 选择性激活记忆信号 $Y$
- **残差连接**：保留原始输入 $X$，避免梯度消失，增强训练稳定性，公式： $O = X + (U \otimes Y)$

###  维度恢复（输出层）
- **LayerNorm + 1×1 卷积**：将 $O$ 恢复至原始嵌入维度 $N$，适配后续 Transformer 模块。
