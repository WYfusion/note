![[Pasted image 20250509193711.png|500]]
上图的理论解释
# 混合块注意力
提出了混合块注意力机制 ，它融合了部分注意力机制和线性注意力机制的优点。但是同年就已经不是很优了。
![[Pasted image 20250512201224.png|500]]
混合块注意力是论文提出的核心创新模块，旨在解决长序列 Transformer 的效率问题。它通过**分块策略**将序列划分为局部块，结合**块内二次注意力**（精确捕捉局部依赖）和**块间线性注意力**（高效建模长距离依赖），实现了线性复杂度与高性能的统一。
## 预处理：分块与特征生成
### 输入形状
输入序列 $x$ 形状为 $[B, T, d]$（批次、序列长度、模型维度），分块后为 $[B, G, C, d]$，其中 $G=T/C$ 为块数，$C$ 为块长（默认 $256$）。
### 特征生成
对每个块 g，通过 GAU 的线性变换生成：
- $U_g$, $V_g \in \mathbb{R}^{C \times e}$（门控机制的输入，$e=2d$ 为扩展维度），
- $Z_g \in \mathbb{R}^{C \times s}$（共享表示，$s=128$ 为缩减维度）， 再通过轻量变换（缩放 + 偏移）生成 4 组查询 / 键：$Q_g^{\text{quad}}, K_g^{\text{quad}}$（块内二次注意力）和 $Q_g^{\text{lin}}, K_g^{\text{lin}}$（块间线性注意力）。
## 块内二次注意力
使用[[GAU门控注意力单元]]，对于每一个块内Token计算其交互：$\hat{V}_g^\mathrm{quad}=\mathrm{relu}^2\left(Q_g^\mathrm{quad}{K_g^{\mathrm{quad}^{\top}}}+b\right)V_g$，$Q_{g}^{\mathrm{quad}}K_{g}^{\mathrm{quad}^{\top}}\in\mathbb{R}^{C\times C}$,相对位置偏差 b（RoPE 编码）
复杂度：单块复杂度 $O(C^2 d)$，总复杂度 $O(G C^2 d) = O(T C d)$（线性于 $T$，因 $C$ 固定）。
## 块间线性注意力
对块 $g$，聚合所有前序块（因果场景）或所有块（非因果）的键值对：$$\begin{aligned}\text{Non-Causal:}&\hat{V}_g^{\mathrm{lin}}=Q_g^{\mathrm{lin}}\left(\sum_{h=1}^GK_h^{\mathrm{lin}\top}V_h\right), && \text{（非因果场景，如BERT）} \\\mathrm{Causal:}&\hat{V}_g^{\mathrm{lin}}=Q_g^{\mathrm{lin}}\left(\sum_{h=1}^{g-1}K_h^{\mathrm{lin}\top}V_h\right).&& \text{（因果场景，如自回归）} \end{aligned}$$
将传统线性注意力的 T 步串行累加（每步处理 1 个 token）转化为 G 步块级累加（每步处理 1 个块），串行步数减少 C 倍（如 T=8K, C=256 时，步数从 8192→32）。
### 内存与计算资源的节省
缓存$K_h^{\mathrm{lin}^\top}V_h\in\mathbb{R}^{s\times e}$(低维矩阵，$s=128$)，内存占用远低于全注意力的 $T \times T$ 矩阵
## 门控输出
### 门控机制
将局部和全局输出相加后，与 $GAU$ 的门控向量 $U_g$ 逐元素相乘（类似 $GLU$ 的门控机制）：$O_g=\left[U_g\odot\left(\hat{V}_g^\mathrm{quad}+\hat{V}_g^\mathrm{lin}\right)\right]W_o$ ，这里的$W_o\in\mathbb{R}^{e\times d}$，为投影矩阵，将维度恢复为 $d$