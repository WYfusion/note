# 常见语音/鸟声特征的系统性说明

**Spectrogram（谱图）**、**MelSpectrogram（梅尔谱）**、**Fbank（滤波器组能量）**、**MFCC**、**Wav2Vec 2.0**、**WavLM**——放在同一条信号处理链上来讲，这样可以看清楚它们是谁的“上游/下游”，以及哪些是**手工构造的频域特征**，哪些是**端到端的深度表示**。

记离散语音/鸟鸣信号为

$x[n], \quad n = 0,1,\dots, N-1$

采样率记为 $f_s$。

---

## 1. 短时傅里叶变换 (STFT) 与基础谱图 Spectrogram

**目的**：把一段时域信号分成很多个短时帧，对每一帧做 FFT，看“这一小段时间里的频率分布”。这是所有下面特征的母体。

1. **分帧 + 加窗**

设帧长为 $L$，帧移为 $H$（通常 $H = \text{hop\_length}$，例如 8 ms），第 $m$ 帧的起点是 $mH$，加窗函数为 $w[\cdot]$（如汉明窗）：

$x_m[n] = x[mH + n]\; w[n], \quad n = 0,1,\dots, L-1$

2. **对每一帧做 DFT/FFT**

$X[m, k] = \sum\limits_{n=0}^{L-1} x_m[n] \, e^{-j 2\pi k n / L}, \quad k = 0,1,\dots, L-1$

3. **谱图 (Spectrogram)**

最常见的是功率谱图或幅度谱图：

- 幅度谱图：
  $S_{\text{mag}}[m,k] = |X[m,k]|$
- 功率谱图：
  $S_{\text{pow}}[m,k] = |X[m,k]|^2$

这张二维矩阵 $(m,k)$ 就是 **Spectrogram**，它是**频率是线性刻度**的（对应普通 FFT 频率 $\frac{k}{L} f_s$）。

---

## 2. MelSpectrogram（梅尔谱）

**核心想法**：人耳/很多动物对频率的分辨率并不是线性，而是低频分得细、高频分得粗。为了“更像听觉”，就把线性频率轴投影到 **Mel 轴** 上。

1. **先做 STFT** 得到功率谱 $S_{\text{pow}}[m,k]$

2. **构造 Mel 滤波器组**

Mel 频率定义（常用一版）：

$\text{mel}(f) = 2595 \log_{10} \left( 1 + \frac{f}{700} \right)$

给定要的 Mel 滤波器个数 $M$（比如 40），在 $[0, f_{\max}]$ 上***等间隔取 Mel 点***，再映回线性频率，按这些频率点搭一个个 **三角滤波器** $H_i[k]$, $i=1,\dots,M$。

3. **把线性谱投到 Mel 轴**

$E_{\text{mel}}[m, i]= \sum\limits_{k=0}^{L-1} S_{\text{pow}}[m,k] \, H_i[k], \quad i = 1,\dots,M$

4. **取 log（可选但常见）**

$\text{MelSpec}[m, i] = \log \big( E_{\text{mel}}[m, i] + \epsilon \big)$

得到的 $\text{MelSpec}$ 就是 **MelSpectrogram**，是一个“**时间 × Mel 通道**”的二维表示，比起线性谱图，它在高频上被压得更粗糙一点，更贴合听觉/鸟听觉建模，也常常更利于后面卷积/Transformer 去做分类。

---

## 3. Fbank（Filterbank Energies，滤波器组能量）

**核心想法**：它其实就是“没有做离散余弦变换 DCT 的 MFCC”，或者说“就是 Mel 滤波器组的 log 能量”。在很多语音/说话人/鸟声任务里，**Fbank 是比 MFCC 更常用的原始特征**，因为它没有再做装饰性变换，保留了更接近谱形的局部性。

步骤跟上面 MelSpectrogram 基本一样，只是最后我们就停在 log-Mel 滤波器能量(同样也是可选的)那里，不再做后面的 DCT：

$\text{Fbank}[m, i] = \log \left( \sum\limits_{k=0}^{L-1} S_{\text{pow}}[m,k] \, H_i[k] + \epsilon \right), \quad i=1,\dots,M$
 **主要差别在哪：**

   ### (1) 表达目的不一样
   - **Mel Spectrogram** 这个说法一般更“图像化”：我要一张“时间 × Mel 频率”的图，可能给人看、给 CNN 看、做可视化。
   - **Fbank** 一般是“给模型的特征向量”，强调的是“这是第 i 个滤波器的 log 能量”。它更像是“特征工程名字”，不是“图的名字。

   ### (2) 细节习惯不一样
   - **Mel Spectrogram**：
     - 有人用 **幅度谱**做 Mel，有人用**功率谱**；
     - 有人取 **log10**，有人取 **ln**；
     - 有人不取 log，直接叫 Mel-spectrogram；
     - 有人会保留“看得懂的频率轴”，会做一点展示用的后处理。
   - **Fbank**：
     - 传统语音识别里几乎总是：**功率谱 → Mel 滤波器组 → log → （有时再做 倒谱均值和方差归一化（Cepstral Mean and Variance Normalization）CMVN）**
     - 维度固定，比如 23、40、80
     - 明确是“每一帧的一维向量”

   所以你可以说：**Fbank 是“Mel 频谱的特征化版本/标准化版本”**。

你也可以理解成：

- **MelSpectrogram**：更图像化的说法
- **Fbank**：更“特征工程”口味的说法

本质上一样：都是“Mel 滤波器组的 log 能量”。



---

## 4. MFCC（Mel-Frequency Cepstral Coefficients，梅尔频率倒谱系数）

**核心想法**：Fbank 还在“频谱域”（Mel 频谱），我们再来一次**离散余弦变换 (DCT-II)**，把频谱的相关性“挤”到前几个系数里，得到一组更“紧凑”的特征，历史上识别器喜欢它，因为维度低、协方差好搞。

1. 有了 Fbank：

$F[m, i] = \text{Fbank}[m, i], \quad i=1,\dots,M$

2. 做 DCT（常用 DCT-II）：

$c[m, n] = \sum_{i=1}^{M} F[m, i] \cos \left( \frac{\pi (i - 0.5) n}{M} \right), \quad n = 0,1,\dots, C-1$

这里 $C$ 是你要的 MFCC 维度（典型 12 或 13，再加能量、Δ、ΔΔ）。

3. 得到的 $c[m,n]$ 就是第 $m$ 帧的第 $n$ 个 **MFCC** 系数。

💡要点：

- **MFCC = log-Mel 滤波器能量 + DCT**；
- 它更“压缩”，但因为 DCT 打散了频域的局部性，对 CNN/attention 不一定是好事；
- 所以深度学习时代很多人宁可用 Fbank/MelSpec 而不是 MFCC。

---

## 5. Wav2Vec 2.0 特征

**类型**：这已经不再是“手工频谱特征”，而是**自监督预训练的表示**（representation / embedding）。它的输入通常是**原始波形**（16 kHz 最常见，可重新采样你 32 kHz 的鸟声到 16 kHz），输出是一串**上下文相关的向量**，可以理解成“模型学会的、对语音结构最敏感的高层特征”。

典型的 Wav2Vec 2.0（以原论文为例）分 3 步：

1. **特征编码器（Feature Encoder）**：一堆 1D 卷积，把原始波形 $x[n]$ 变成低帧率的潜在表示 $\mathbf{z}_t \in \mathbb{R}^{d_z}$

   输入：$x \in \mathbb{R}^T$

   $\mathbf{z}_t = f_{\text{enc}}(x)[t], \quad t = 1,\dots, T'$

   这里的 $f_{\text{enc}}$ 就像是一个学出来的“更聪明的滤波器组/频谱提取器”，它不是固定的 Mel，而是卷积核里学出来的。

2. **量化模块（Gumbel-Softmax / codebook）**：从 $\mathbf{z}_t$ 里挑一个离散代码 $\mathbf{q}_t$，作为预测目标，用来做自监督对比学习：

   $\mathbf{q}_t = \text{quantize}(\mathbf{z}_t)$

3. **上下文网络（Transformer）**：把整段序列扔进一个 Transformer，得到**上下文后的表示** $\mathbf{c}_t$：

   $\mathbf{c}_t = f_{\text{context}}(\mathbf{z}_{1:T'})[t], \quad \mathbf{c}_t \in \mathbb{R}^{d_c}$

   这个 $\mathbf{c}_t$ 就是我们平时说的“Wav2Vec 2.0 的特征/表示”。它是**时序上稀疏（帧率低）**、**维度高（几百）**、**带上下文**、**对说话人/语音内容都比较敏感**的表示。

训练目标（InfoNCE 风格）可以写成：

$\mathcal{L}= - \sum_{t \in \mathcal{M}} \log \frac{\exp\big( \text{sim}(\mathbf{c}_t, \mathbf{q}_t) / \kappa \big)}{\sum_{\mathbf{q} \in \mathcal{Q}_t} \exp\big( \text{sim}(\mathbf{c}_t, \mathbf{q}) / \kappa \big)}$

- $\mathcal{M}$ 是被 mask 的时间步
- $\text{sim}(\cdot,\cdot)$ 是相似度（点积）
- $\kappa$ 是温度
- $\mathcal{Q}_t$ 包含正确的量化向量 $\mathbf{q}_t$ 和一堆负样本

**结论**：Wav2Vec 2.0 的“特征”不是固定公式算出来的频谱，而是**卷积 + Transformer 自监督学出来的上下文语音表征**，可以直接拿来做下游任务（分类、识别、分段、鸟种识别）。

---

## 6. WavLM 特征

**WavLM** 可以看成是“Wav2Vec 2.0 在 Microsoft 版本的增强版”：依然是**自监督、端到端、输入原始波形**，但它的**预训练任务更多样/更贴近下游语音任务**，尤其是噪声/说话人建模。

核心可以概念化成：

1. 输入还是波形 $x$
2. 特征编码器还是一串卷积，得到 $\mathbf{z}_t$
3. Transformer 得到上下文 $\mathbf{c}_t$
4. 预训练时加入了**语音完形填空 + 带噪/说话人感知的建模目标**，使得 $\mathbf{c}_t$：
   - 对说话人信息更敏感
   - 对语音分割/重建更鲁棒
   - 在噪声环境下表现更好

可以抽象写成多任务自监督目标：

$\mathcal{L}_{\text{WavLM}} = \lambda_1 \mathcal{L}_{\text{mask-pred}} + \lambda_2 \mathcal{L}_{\text{denoise}} + \lambda_3 \mathcal{L}_{\text{speaker-aware}} + \dots$

最终得到的 $\mathbf{c}_t^{\text{WavLM}}$ 就是我们说的 **WavLM 表征**，在你要做的“鸟声识别/鸟种分类/端点检测鲁棒化”里，它就是一个**已经在超大语音数据上预训练过的、带上下文的、可以微调的上游 embedding**，用法跟 “把 log-Mel 扔给 CNN” 的时代已经不同了。

---

## 7. 小结对比

- **Spectrogram**：最基础的时频能量图；频率是**线性**的；公式最简单；很多算法第一步。
- **MelSpectrogram**：= 线性谱图 + **Mel 滤波器组**；频率变成“听觉刻度”；结果更稠密、对高频不敏感。
- **Fbank**：= MelSpectrogram 的“特征版”；本质是 **(log-)Mel 滤波器能量**；是很多深度语音/说话人/鸟声系统的默认输入。
- **MFCC**：= Fbank + **DCT**；把频谱的相关性挤到前面几个系数里，便于老式建模；但会丢一些局部结构。
- **Wav2Vec 2.0 / WavLM**：不再是手工特征，而是**端到端自监督预训练的表示**，输入是波形，输出是一串高维上下文向量，靠大模型自己学到“应该关注什么”。

---

## 8. 公式一览（便于你拷到文档里）

1. **STFT**
   $X[m, k] = \sum_{n=0}^{L-1} x[mH + n] w[n] e^{-j 2\pi k n / L}$

2. **功率谱图**
   $S_{\text{pow}}[m,k] = |X[m,k]|^2$

3. **Mel 频率**
   $\text{mel}(f) = 2595 \log_{10} \left( 1 + \frac{f}{700} \right)$

4. **Mel 滤波器组投影**
   $E_{\text{mel}}[m, i]   = \sum_{k=0}^{L-1} S_{\text{pow}}[m,k] \, H_i[k]$

5. **Fbank（log-Mel 能量）**
   $\text{Fbank}[m, i] = \log \left( E_{\text{mel}}[m, i] + \epsilon \right)$

6. **MFCC（DCT-II）**
   $c[m, n] = \sum_{i=1}^{M} \text{Fbank}[m, i]   \cos \left( \frac{\pi (i - 0.5) n}{M} \right)$

7. **Wav2Vec 2.0 特征抽象**
   $\mathbf{z}_t = f_{\text{enc}}(x)[t], \quad  \mathbf{c}_t = f_{\text{context}}(\mathbf{z}_{1:T'})[t]$

8. **对比学习损失（示意）**
   $\mathcal{L}  = - \sum\limits_{t \in \mathcal{M}} \log    \frac{     \exp\big( \text{sim}(\mathbf{c}_t, \mathbf{q}_t) / \kappa \big)   }{     \sum_{\mathbf{q} \in \mathcal{Q}_t} \exp\big( \text{sim}(\mathbf{c}_t, \mathbf{q}) / \kappa \big)   }$

---

## 9. 简单对应到你的鸟声场景

- 你要做 **最低频率检测 / 能量百分位** → 最好先有 **谱图或 Mel/Fbank**，你能在频率轴上做能量积分；
- 你要做 **179 类鸟种分类** → 直接用 **Fbank / MelSpec** 喂 CNN/ECAPA-TDNN/NeXt-TDNN 是最稳的；
- 你要追求 **更高鲁棒/更强语义** → 可以考虑把音频下采样到 16 kHz，过一遍 **WavLM Base / Wav2Vec2 Base**，取它的上下文层输出再喂分类器。

