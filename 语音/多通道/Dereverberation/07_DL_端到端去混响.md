# 端到端深度学习去混响

## 1. 概述

**端到端去混响**直接在时域或频域学习从混响语音到干净语音的映射，无需显式的特征提取或掩码估计。

### 1.1 优势

- 避免手工特征设计
- 联合优化所有模块
- 更好的性能上限

---

## 2. 时域端到端方法

### 2.1 Conv-TasNet架构

**编码器**：
$$\mathbf{w} = \text{Encoder}(y)$$

**分离网络**：
$$\mathbf{m} = \text{TCN}(\mathbf{w})$$

**解码器**：
$$\hat{x} = \text{Decoder}(\mathbf{m} \odot \mathbf{w})$$

```python
class ConvTasNet(nn.Module):
    def __init__(self, N=512, L=16, B=128, H=512, P=3, X=8, R=3):
        super().__init__()
        
        # 编码器
        self.encoder = nn.Conv1d(1, N, L, stride=L//2, bias=False)
        
        # 分离网络
        self.separator = TemporalConvNet(N, B, H, P, X, R)
        
        # 解码器
        self.decoder = nn.ConvTranspose1d(N, 1, L, stride=L//2, bias=False)
    
    def forward(self, mixture):
        # mixture: [B, 1, T]
        w = self.encoder(mixture)  # [B, N, K]
        m = self.separator(w)      # [B, N, K]
        s = m * w                  # [B, N, K]
        output = self.decoder(s)   # [B, 1, T]
        return output
```

### 2.2 DCCRN架构

**复数卷积**：
$$\mathbf{z} = \mathbf{W}_r * \mathbf{x}_r - \mathbf{W}_i * \mathbf{x}_i + j(\mathbf{W}_r * \mathbf{x}_i + \mathbf{W}_i * \mathbf{x}_r)$$

---

## 3. 频域端到端方法

### 3.1 DCUNET架构

**U-Net结构**：
```
编码器: Conv → BN → ReLU → MaxPool
解码器: ConvTranspose → BN → ReLU → Concat
```

```python
class DCUNET(nn.Module):
    def __init__(self):
        super().__init__()
        
        # 编码器
        self.enc1 = self._conv_block(2, 32)
        self.enc2 = self._conv_block(32, 64)
        self.enc3 = self._conv_block(64, 128)
        
        # 瓶颈
        self.bottleneck = self._conv_block(128, 256)
        
        # 解码器
        self.dec3 = self._deconv_block(256, 128)
        self.dec2 = self._deconv_block(256, 64)
        self.dec1 = self._deconv_block(128, 32)
        
        # 输出
        self.out = nn.Conv2d(64, 2, 1)
    
    def _conv_block(self, in_ch, out_ch):
        return nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True)
        )
    
    def _deconv_block(self, in_ch, out_ch):
        return nn.Sequential(
            nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True)
        )
    
    def forward(self, x):
        # x: [B, 2, F, T] (实部和虚部)
        
        # 编码
        e1 = self.enc1(x)
        e2 = self.enc2(F.max_pool2d(e1, 2))
        e3 = self.enc3(F.max_pool2d(e2, 2))
        
        # 瓶颈
        b = self.bottleneck(F.max_pool2d(e3, 2))
        
        # 解码
        d3 = self.dec3(b)
        d3 = torch.cat([d3, e3], dim=1)
        
        d2 = self.dec2(d3)
        d2 = torch.cat([d2, e2], dim=1)
        
        d1 = self.dec1(d2)
        d1 = torch.cat([d1, e1], dim=1)
        
        # 输出
        out = self.out(d1)
        return out
```

---

## 4. Transformer架构

### 4.1 自注意力机制

**多头注意力**：
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

### 4.2 Speech-Transformer

```python
class SpeechTransformer(nn.Module):
    def __init__(self, d_model=512, nhead=8, num_layers=6):
        super().__init__()
        
        self.embedding = nn.Linear(257, d_model)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model, nhead, dim_feedforward=2048
        )
        self.transformer = nn.TransformerEncoder(
            encoder_layer, num_layers
        )
        
        self.output = nn.Linear(d_model, 257)
    
    def forward(self, x):
        # x: [B, T, F]
        x = self.embedding(x)
        x = x.transpose(0, 1)  # [T, B, d_model]
        x = self.transformer(x)
        x = x.transpose(0, 1)  # [B, T, d_model]
        x = self.output(x)
        return torch.sigmoid(x)
```

---

## 5. 损失函数设计

### 5.1 多尺度损失

$$\mathcal{L} = \sum_{i} w_i \mathcal{L}_i(\hat{x}, x)$$

包括：
- 时域MSE
- 频域MSE
- 感知损失

### 5.2 对抗损失

**生成器**：
$$\mathcal{L}_G = \mathcal{L}_{\text{recon}} + \lambda \mathcal{L}_{\text{adv}}$$

**判别器**：
$$\mathcal{L}_D = -\mathbb{E}[\log D(x)] - \mathbb{E}[\log(1-D(\hat{x}))]$$

---

## 6. 训练技巧

### 6.1 预训练策略

1. 在大规模数据上预训练
2. 在目标数据上微调

### 6.2 多任务学习

同时学习：
- 去混响
- 去噪
- 语音增强

---

## 7. 性能对比

| 方法 | PESQ | STOI | 参数量 |
|------|------|------|--------|
| WPE | 2.1 | 0.85 | - |
| DNN | 2.5 | 0.88 | 5M |
| Conv-TasNet | 2.8 | 0.91 | 8M |
| DCUNET | 3.0 | 0.93 | 12M |

---

## 8. 实际部署

### 8.1 模型压缩

**量化**：
```python
model_int8 = torch.quantization.quantize_dynamic(
    model, {nn.Linear, nn.LSTM}, dtype=torch.qint8
)
```

**剪枝**：
```python
import torch.nn.utils.prune as prune
prune.l1_unstructured(module, name='weight', amount=0.3)
```

### 8.2 实时推理

**流式处理**：
- 使用因果卷积
- 分块处理
- 重叠相加

---

## 参考文献

1. Luo, Y., & Mesgarani, N. (2019). "Conv-TasNet: Surpassing ideal time-frequency magnitude masking for speech separation." IEEE/ACM TASLP.

2. Choi, H. S., et al. (2019). "Phase-aware speech enhancement with deep complex U-Net." ICLR.

3. Pandey, A., & Wang, D. (2019). "A new framework for CNN-based speech enhancement in the time domain." IEEE/ACM TASLP.

4. Défossez, A., et al. (2020). "Real time speech enhancement in the waveform domain." Interspeech.
