# 贝叶斯滤波

## 1. 概述

**贝叶斯滤波 (Bayesian Filtering)** 是基于贝叶斯理论的递归状态估计方法，是所有滤波算法的理论基础。

### 1.1 核心思想

通过递归地结合：
- **预测**：利用运动模型预测下一时刻状态
- **更新**：利用观测修正预测

---

## 2. 贝叶斯理论基础

### 2.1 贝叶斯公式

**基本形式**：

$$p(x|z) = \frac{p(z|x)p(x)}{p(z)}$$

其中：
- $p(x|z)$：后验概率
- $p(z|x)$：似然函数
- $p(x)$：先验概率
- $p(z)$：证据（归一化常数）

**递归形式**：

$$p(\mathbf{x}_t | \mathbf{z}_{1:t}) = \frac{p(\mathbf{z}_t | \mathbf{x}_t) p(\mathbf{x}_t | \mathbf{z}_{1:t-1})}{p(\mathbf{z}_t | \mathbf{z}_{1:t-1})}$$

### 2.2 马尔可夫假设

**一阶马尔可夫性**：

$$p(\mathbf{x}_t | \mathbf{x}_{0:t-1}) = p(\mathbf{x}_t | \mathbf{x}_{t-1})$$

当前状态只依赖于前一时刻状态。

**观测独立性**：

$$p(\mathbf{z}_t | \mathbf{x}_{0:t}, \mathbf{z}_{1:t-1}) = p(\mathbf{z}_t | \mathbf{x}_t)$$

当前观测只依赖于当前状态。

---

## 3. 贝叶斯滤波递归

### 3.1 预测步骤

**Chapman-Kolmogorov方程**：

$$p(\mathbf{x}_t | \mathbf{z}_{1:t-1}) = \int p(\mathbf{x}_t | \mathbf{x}_{t-1}) p(\mathbf{x}_{t-1} | \mathbf{z}_{1:t-1}) d\mathbf{x}_{t-1}$$

**物理意义**：
- 利用状态转移概率 $p(\mathbf{x}_t | \mathbf{x}_{t-1})$
- 对所有可能的前一状态积分
- 得到预测分布

### 3.2 更新步骤

**贝叶斯更新**：

$$p(\mathbf{x}_t | \mathbf{z}_{1:t}) = \frac{p(\mathbf{z}_t | \mathbf{x}_t) p(\mathbf{x}_t | \mathbf{z}_{1:t-1})}{p(\mathbf{z}_t | \mathbf{z}_{1:t-1})}$$

其中归一化常数：

$$p(\mathbf{z}_t | \mathbf{z}_{1:t-1}) = \int p(\mathbf{z}_t | \mathbf{x}_t) p(\mathbf{x}_t | \mathbf{z}_{1:t-1}) d\mathbf{x}_t$$

**物理意义**：
- 利用似然函数 $p(\mathbf{z}_t | \mathbf{x}_t)$
- 修正预测分布
- 得到后验分布

### 3.3 完整递归

```
初始化: p(x_0)
  ↓
预测: p(x_t | z_{1:t-1})
  ↓
观测: z_t
  ↓
更新: p(x_t | z_{1:t})
  ↓
t = t + 1
```

---

## 4. 最优估计

### 4.1 最大后验估计 (MAP)

$$\hat{\mathbf{x}}_t^{\text{MAP}} = \arg\max_{\mathbf{x}_t} p(\mathbf{x}_t | \mathbf{z}_{1:t})$$

**特点**：
- 选择后验概率最大的状态
- 对应后验分布的峰值

### 4.2 最小均方误差估计 (MMSE)

$$\hat{\mathbf{x}}_t^{\text{MMSE}} = \mathbb{E}[\mathbf{x}_t | \mathbf{z}_{1:t}] = \int \mathbf{x}_t p(\mathbf{x}_t | \mathbf{z}_{1:t}) d\mathbf{x}_t$$

**特点**：
- 后验分布的期望
- 最小化均方误差

**协方差**：

$$\mathbf{P}_t = \mathbb{E}[(\mathbf{x}_t - \hat{\mathbf{x}}_t)(\mathbf{x}_t - \hat{\mathbf{x}}_t)^T | \mathbf{z}_{1:t}]$$

### 4.3 高斯情况

当后验分布为高斯时：

$$p(\mathbf{x}_t | \mathbf{z}_{1:t}) = \mathcal{N}(\mathbf{x}_t; \hat{\mathbf{x}}_t, \mathbf{P}_t)$$

MAP和MMSE估计相同：

$$\hat{\mathbf{x}}_t^{\text{MAP}} = \hat{\mathbf{x}}_t^{\text{MMSE}} = \hat{\mathbf{x}}_t$$

---

## 5. 线性高斯情况

### 5.1 模型假设

**线性状态方程**：

$$\mathbf{x}_t = \mathbf{F}_t \mathbf{x}_{t-1} + \mathbf{w}_t, \quad \mathbf{w}_t \sim \mathcal{N}(\mathbf{0}, \mathbf{Q}_t)$$

**线性观测方程**：

$$\mathbf{z}_t = \mathbf{H}_t \mathbf{x}_t + \mathbf{v}_t, \quad \mathbf{v}_t \sim \mathcal{N}(\mathbf{0}, \mathbf{R}_t)$$

### 5.2 预测分布

**先验分布**：

$$p(\mathbf{x}_{t-1} | \mathbf{z}_{1:t-1}) = \mathcal{N}(\mathbf{x}_{t-1}; \hat{\mathbf{x}}_{t-1}, \mathbf{P}_{t-1})$$

**预测分布**：

$$p(\mathbf{x}_t | \mathbf{z}_{1:t-1}) = \mathcal{N}(\mathbf{x}_t; \hat{\mathbf{x}}_{t|t-1}, \mathbf{P}_{t|t-1})$$

其中：

$$\hat{\mathbf{x}}_{t|t-1} = \mathbf{F}_t \hat{\mathbf{x}}_{t-1}$$

$$\mathbf{P}_{t|t-1} = \mathbf{F}_t \mathbf{P}_{t-1} \mathbf{F}_t^T + \mathbf{Q}_t$$

### 5.3 更新分布

**似然函数**：

$$p(\mathbf{z}_t | \mathbf{x}_t) = \mathcal{N}(\mathbf{z}_t; \mathbf{H}_t \mathbf{x}_t, \mathbf{R}_t)$$

**后验分布**：

$$p(\mathbf{x}_t | \mathbf{z}_{1:t}) = \mathcal{N}(\mathbf{x}_t; \hat{\mathbf{x}}_t, \mathbf{P}_t)$$

其中：

$$\mathbf{K}_t = \mathbf{P}_{t|t-1} \mathbf{H}_t^T (\mathbf{H}_t \mathbf{P}_{t|t-1} \mathbf{H}_t^T + \mathbf{R}_t)^{-1}$$

$$\hat{\mathbf{x}}_t = \hat{\mathbf{x}}_{t|t-1} + \mathbf{K}_t (\mathbf{z}_t - \mathbf{H}_t \hat{\mathbf{x}}_{t|t-1})$$

$$\mathbf{P}_t = (\mathbf{I} - \mathbf{K}_t \mathbf{H}_t) \mathbf{P}_{t|t-1}$$

这就是**卡尔曼滤波**！

---

## 6. 非线性非高斯情况

### 6.1 挑战

- 预测和更新的积分无解析解
- 后验分布不再是高斯分布
- 需要近似方法

### 6.2 近似方法

**参数化近似**：
- 扩展卡尔曼滤波 (EKF)
- 无迹卡尔曼滤波 (UKF)

**非参数化近似**：
- 粒子滤波 (PF)
- 网格滤波

---

## 7. 信息形式

### 7.1 信息矩阵

**信息矩阵**：

$$\mathbf{Y}_t = \mathbf{P}_t^{-1}$$

**信息向量**：

$$\mathbf{y}_t = \mathbf{P}_t^{-1} \hat{\mathbf{x}}_t$$

### 7.2 信息滤波器

**预测**：

$$\mathbf{Y}_{t|t-1}^{-1} = \mathbf{F}_t \mathbf{Y}_{t-1}^{-1} \mathbf{F}_t^T + \mathbf{Q}_t$$

**更新**：

$$\mathbf{Y}_t = \mathbf{Y}_{t|t-1} + \mathbf{H}_t^T \mathbf{R}_t^{-1} \mathbf{H}_t$$

$$\mathbf{y}_t = \mathbf{y}_{t|t-1} + \mathbf{H}_t^T \mathbf{R}_t^{-1} \mathbf{z}_t$$

**优势**：
- 多传感器融合更简单
- 分布式实现更容易

---

## 8. 实现示例

```python
import numpy as np
import matplotlib.pyplot as plt

class BayesianFilter:
    """贝叶斯滤波基类"""
    
    def __init__(self):
        self.x_est = None  # 状态估计
        self.P_est = None  # 协方差估计
    
    def initialize(self, x0, P0):
        """初始化"""
        self.x_est = x0
        self.P_est = P0
    
    def predict(self):
        """预测步骤"""
        raise NotImplementedError
    
    def update(self, z):
        """更新步骤"""
        raise NotImplementedError


class KalmanFilter(BayesianFilter):
    """卡尔曼滤波器（线性高斯情况）"""
    
    def __init__(self, F, H, Q, R):
        """
        参数:
            F: 状态转移矩阵
            H: 观测矩阵
            Q: 过程噪声协方差
            R: 观测噪声协方差
        """
        super().__init__()
        self.F = F
        self.H = H
        self.Q = Q
        self.R = R
    
    def predict(self):
        """预测步骤"""
        # 状态预测
        self.x_pred = self.F @ self.x_est
        
        # 协方差预测
        self.P_pred = self.F @ self.P_est @ self.F.T + self.Q
        
        return self.x_pred, self.P_pred
    
    def update(self, z):
        """更新步骤"""
        # 创新（残差）
        y = z - self.H @ self.x_pred
        
        # 创新协方差
        S = self.H @ self.P_pred @ self.H.T + self.R
        
        # 卡尔曼增益
        K = self.P_pred @ self.H.T @ np.linalg.inv(S)
        
        # 状态更新
        self.x_est = self.x_pred + K @ y
        
        # 协方差更新
        self.P_est = (np.eye(len(self.x_est)) - K @ self.H) @ self.P_pred
        
        return self.x_est, self.P_est
    
    def get_likelihood(self, z):
        """计算观测似然"""
        y = z - self.H @ self.x_pred
        S = self.H @ self.P_pred @ self.H.T + self.R
        
        # 多元高斯概率密度
        n = len(z)
        det_S = np.linalg.det(S)
        inv_S = np.linalg.inv(S)
        
        likelihood = (1.0 / np.sqrt((2*np.pi)**n * det_S)) * \
                     np.exp(-0.5 * y.T @ inv_S @ y)
        
        return likelihood


def example_bayesian_filtering():
    """贝叶斯滤波示例"""
    
    # 系统参数
    dt = 0.1
    F = np.array([[1, 0, dt, 0],
                  [0, 1, 0, dt],
                  [0, 0, 1, 0],
                  [0, 0, 0, 1]])
    
    H = np.array([[1, 0, 0, 0],
                  [0, 1, 0, 0]])
    
    Q = 0.1 * np.array([[dt**3/3, 0, dt**2/2, 0],
                        [0, dt**3/3, 0, dt**2/2],
                        [dt**2/2, 0, dt, 0],
                        [0, dt**2/2, 0, dt]])
    
    R = 0.5 * np.eye(2)
    
    # 创建滤波器
    kf = KalmanFilter(F, H, Q, R)
    
    # 初始化
    x0 = np.array([0, 0, 1, 0.5])
    P0 = np.diag([1, 1, 0.5, 0.5])
    kf.initialize(x0, P0)
    
    # 生成真实轨迹
    T = 100
    x_true = np.zeros((4, T))
    z_obs = np.zeros((2, T))
    x_true[:, 0] = x0
    
    for t in range(1, T):
        # 真实状态演化
        w = np.random.multivariate_normal(np.zeros(4), Q)
        x_true[:, t] = F @ x_true[:, t-1] + w
        
        # 观测
        v = np.random.multivariate_normal(np.zeros(2), R)
        z_obs[:, t] = H @ x_true[:, t] + v
    
    # 滤波
    x_est = np.zeros((4, T))
    P_trace = np.zeros(T)
    x_est[:, 0] = x0
    P_trace[0] = np.trace(P0)
    
    for t in range(1, T):
        # 预测
        kf.predict()
        
        # 更新
        kf.update(z_obs[:, t])
        
        x_est[:, t] = kf.x_est
        P_trace[t] = np.trace(kf.P_est)
    
    # 可视化
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # 轨迹
    axes[0, 0].plot(x_true[0, :], x_true[1, :], 'g-', label='True', linewidth=2)
    axes[0, 0].plot(z_obs[0, :], z_obs[1, :], 'r.', label='Observation', alpha=0.5)
    axes[0, 0].plot(x_est[0, :], x_est[1, :], 'b--', label='Estimate', linewidth=2)
    axes[0, 0].set_xlabel('X Position (m)')
    axes[0, 0].set_ylabel('Y Position (m)')
    axes[0, 0].set_title('Trajectory')
    axes[0, 0].legend()
    axes[0, 0].grid(True)
    
    # X位置误差
    axes[0, 1].plot(x_true[0, :] - x_est[0, :])
    axes[0, 1].set_xlabel('Time Step')
    axes[0, 1].set_ylabel('X Error (m)')
    axes[0, 1].set_title('X Position Error')
    axes[0, 1].grid(True)
    
    # Y位置误差
    axes[1, 0].plot(x_true[1, :] - x_est[1, :])
    axes[1, 0].set_xlabel('Time Step')
    axes[1, 0].set_ylabel('Y Error (m)')
    axes[1, 0].set_title('Y Position Error')
    axes[1, 0].grid(True)
    
    # 协方差迹
    axes[1, 1].plot(P_trace)
    axes[1, 1].set_xlabel('Time Step')
    axes[1, 1].set_ylabel('Trace(P)')
    axes[1, 1].set_title('Covariance Trace')
    axes[1, 1].grid(True)
    
    plt.tight_layout()
    plt.show()
    
    # 计算RMSE
    rmse_x = np.sqrt(np.mean((x_true[0, :] - x_est[0, :])**2))
    rmse_y = np.sqrt(np.mean((x_true[1, :] - x_est[1, :])**2))
    print(f"RMSE X: {rmse_x:.3f} m")
    print(f"RMSE Y: {rmse_y:.3f} m")

# example_bayesian_filtering()
```

---

## 9. 性能评估

### 9.1 均方误差 (MSE)

$$\text{MSE} = \mathbb{E}[\|\mathbf{x}_t - \hat{\mathbf{x}}_t\|^2]$$

### 9.2 均方根误差 (RMSE)

$$\text{RMSE} = \sqrt{\text{MSE}}$$

### 9.3 归一化估计误差平方 (NEES)

$$\text{NEES}_t = (\mathbf{x}_t - \hat{\mathbf{x}}_t)^T \mathbf{P}_t^{-1} (\mathbf{x}_t - \hat{\mathbf{x}}_t)$$

**一致性检验**：
- 如果滤波器一致，$\text{NEES}_t \sim \chi^2_{n_x}$
- 可用于检测滤波器是否过于自信或不自信

---

## 10. 总结

### 10.1 关键概念

1. **递归结构**：预测-更新循环
2. **贝叶斯框架**：概率推理
3. **最优性**：MMSE意义下最优
4. **高斯情况**：卡尔曼滤波

### 10.2 实际考虑

- **初始化**：合理的初始状态和协方差
- **模型匹配**：模型与实际系统的匹配程度
- **参数调优**：Q和R的选择
- **数值稳定性**：协方差矩阵的正定性

---

## 参考文献

1. Jazwinski, A. H. (1970). "Stochastic Processes and Filtering Theory." Academic Press.

2. Anderson, B. D., & Moore, J. B. (1979). "Optimal Filtering." Prentice-Hall.

3. Särkkä, S. (2013). "Bayesian Filtering and Smoothing." Cambridge University Press.
