Google于2018年发布的基于Transformer编码器的模型，首次实现双向上下文建模。通过大规模无监督预训练（如掩码语言模型MLM和下一句预测NSP），BERT在11项NLP任务中刷新了SOTA。

BERT是实现填空的作用，但是可以用在其他下游任务上，这个任务可以不一定和填空有关，甚至无关也可以。

---
`BERT` 是 `transformer` 的 `Encoder` 架构，输入一排的向量，输出就有一排同长向量。

#### BERT在预训练中的实现：
这个预训练的作用其实就是一个初始化的过程，为了更好的用于下游任务中去。

1. BERT对输入序列中的每个token独立评估，以15%的概率决定是否掩码，注意这种策略是离散位置单个化的token。
2. 将被盖住的位置处换为新的token，这15%的token中，并非所有都被替换为掩码符号，而是进一步分为三种情况：
    1. 80%的概率替换为[MASK]符号（特定掩蔽token）；
    2. 10%的概率替换为随机token；
    3. 10%的概率保持原token不变（保留上下文信息以缓解预训练与微调的不匹配）
也即先随机选中被替换token --> 再随机挑选替换token将被选中的token替换即可。
3. 得到新的seq进入 BERT 中去得到另一个seq，将被掩码位置的输出向量（隐藏状态）经过一个**线性层（无激活函数）**，映射到词表维度，**分类目标**：直接预测被掩码token在词表中的原始ID
4. 计算预测概率分布与真实标签（one-hot编码的原始token ID）之间的交叉熵。
简而言之就是做一个分类，分类目标是原被掩蔽的token所代表的特征向量。分类个数等于词表大小。
![[Pasted image 20250317193719.png|1200]]
### BERT的应用
虽然BERT被设计时貌似只能做填空题，但是可以被分化为各式各样的任务。
### BERT与De-noising Auto-encode相似
![[Pasted image 20250318133144.png|600]]


#### 使用fine-tune微调BERT的好处
明显比随机初始化的BERT好训练的多，并且训练完成后依旧比随机初始化的BERT更好。图中虚线的是随机初始化的，实线的是Pre-train后fine-tune的。
![[Pasted image 20250318141135.png|600]]
fine-tune的示例：
Case1：句子情感倾向
Case2：句子内词性辨析
Case3：两句子关系辨别。前提句与结论句关系辨析(可以用作判定发言立场)。
Case4：问答模型输入Document和Question，输出s、e整数数字，答案是Document从s到e的部分。
![[Pasted image 20250317212727.png|1000]]
#### BERT为何有效
不同的句子下的相同token之间的含义很有可能不一样。例如水果苹果和苹果电脑，可见不同上下文下，苹果含义不同。
![[Pasted image 20250317223806.png|800]]
![[Pasted image 20250317223711.png|800]]
而BERT在学习的过程中就学到了上下文的信息。所以说BERT所抽出来的向量就是可以说是Contextualized word embedding。

当然，BERT有效的可解释性比较差，可能很不相关的用了也会有效果。

对于语言填空方面，也有奇效，甚至跨语言下也可以表现的很好，但是需要预训练的资料量够高，另外可能会出现“假收敛现象"![[Pasted image 20250317225326.png|600]]![[Pasted image 20250317225749.png|600]]

- **BART**：双向编码器-自回归解码器的融合下面是不同的替换策略
    - **核心架构**： BART采用**编码器-解码器架构**，编码器类似BERT的双向Transformer，解码器类似GPT的单向自回归Transformer。这种设计使其既能理解上下文（编码器），又能生成连贯文本（解码器）。
    - 特定掩蔽token、随机token替换
    - 删除token
    - 更换句子顺序
    - 打乱不同句子token
    - 同时掩蔽不同句子的tokon
-  **MASS**：面向生成的统一掩码序列建模
    **核心架构**： MASS同样为**编码器-解码器结构**，但专注于**序列到序列的生成任务**。
    - **连续跨度掩码**：对输入序列中连续的k个token进行掩码（k为输入长度的50%），解码器需预测整个掩码跨度。
    - **自回归预测**：编码器处理被掩码的输入，解码器自回归生成被掩码的片段，强化对长距离依赖的建模。
- **BERT**：通过随机单token掩码平衡上下文学习与噪声鲁棒性，奠定双向理解基础；
- **BART**：融合生成与理解能力，通过多样化噪声模拟真实数据损坏场景，适应复杂任务；
- **MASS**：以生成任务为核心，通过长跨度掩码强制模型学习序列全局依赖
![[Pasted image 20250317220008.png|600]]