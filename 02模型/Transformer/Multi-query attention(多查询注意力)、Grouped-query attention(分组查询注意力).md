https://doi.org/10.48550/arXiv.2305.13245

图1![[Pasted image 20251227215047.png|800]]
图二![[Pasted image 20251227214523.png|800]]
**多头注意力机制**（MHA）包含 H 个查询头、键头和值头。
**分组查询注意力机制**（GQA）则为每个查询头组共享同一个键头和值头，介于多头注意力机制和多查询注意力机制之间。
**多查询注意力机制**（MQA）在所有查询头之间共享同一个键头和值头。

### 多头注意力机制
[[Multi-Head Self-Attention#^59530b|头选取]]
采用多头注意力机制（MHA）的语言模型检查点可以通过升级训练（Komatsuzaki et al., 2022）来使用多任务注意力机制（MQA），且仅需少量原始训练计算资源。
### 多查询注意力机制
**优点**：可以显著降低加载键值对所需的内存带宽，它使用多个查询头，但每个键和值都使用单个头。
**缺点**：多查询注意力机制（MQA）可能导致质量下降和训练不稳定，而且训练针对质量和推理分别优化的独立模型可能并不现实。
