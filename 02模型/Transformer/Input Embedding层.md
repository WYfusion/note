**向量化表示**：将离散的输入数据（通常是序列中的单词或字符）转换为连续的向量表示。这些向量捕捉了输入的语义信息。

**维度转换**：输入的符号通常是通过查找嵌入矩阵（embedding matrix）转换为固定维度的向量。这样做可以确保所有输入符号的表示具有相同的维度，便于后续的计算。

**语义捕捉**：嵌入层通过学习将语义相似的符号映射到相近的向量空间位置，从而提高模型对输入的理解能力。

**位置编码**：在Transformer中，输入嵌入通常会与位置编码（positional encoding）结合，以保留输入序列的位置信息。这是因为Self-Attention本身不考虑顺序。

------

> [!tip] Title
> 建议看Self-Attention中的Q、K、V。注意下面的公式和Self-Attention中不同的原因是下面的做了转置运算，一行为一个特征向量形式

**1.**  $Q、K、V$的获取


- 将得到的所有的$a$，即$a_1$和$a_2$经过相同的三个**参数矩阵**$W^q、W^k、W^v$，分别得到$q^1、k^1、v^1$；$q^2、k^2、v^2$。
- 将$q、k、v$按列拼接得到$Q、K、V$矩阵，形状是$(x$个数$\times$ $x$长度$)$
- $q$是$query$作用是匹配$key$
![[image-20241120230637051 1.png]]

**2.** 得到$Q、K、V$矩阵后进行$\text{Attention}(Q,K,V)=\text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$，$\text{softmax}(\frac{QK^T}{\sqrt{d_k}})$部分运算的原理如下图所示。

- 注意下图中蓝框部分$q^1$与$k^i$之间是点乘的关系，也即对应元素相乘积，但是点乘完了要相加求和再除以$\sqrt d$，也其实是方便于后续矩阵运算，也可以认为是两个向量[[内积#^56411a|内积]]
- $d$是$k$的维度大小，但是$k$是一个向量，所以说就是$k$的元素数
- 注意$k^1$和$k^2$是按列写的矩阵形式，也即$K$的转置$K^T$
- 然后对$QK^T$运算结果对行进行Soft-max运算

![[image-20241120230908158.png]]

**3.** 最后进行$V$的右乘运算，
运算原理如下所示：
![[image-20241120234510146 1.png]]
