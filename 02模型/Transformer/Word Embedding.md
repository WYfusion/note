让机器认识句子的方法：先对这个句子每个部分进行分词$\text{token}$，然后对每个分词$\text{token}$出来的单词都进行$\text{one-hot}$编码，那么下次在遇到相应的单词时，就可以通过查这个编码，去获得这个句子的意思了。

今天天气不错，我要去看电影。可以通过分词划分为，**今天**/**天气**/**不错**/，/**我**/**要去**/**看**/**电影**这8个词，那么我们对这八个词进行one-hot编码，比如**今天**可以得到编码为[1,0,0,0,0,0,0,0],而**我**则被编码为[0,0,0,0,1,0,0,0].

但是会有两方面的问题：

1. 中文的词太多了
2. 使用$\text{one-hot}$编码，词与词之间的相似性被丢弃了。

于是使用了$\text{Word Embedding}$将每一个$\text{token}$的独热码进一步映射为$\text{N}$维$\text{(embedded\_dim)}$的空间上的点。

例如：**今天**可以再次后编码为[0.1，0.2，0.3]， **我**则编码为[0.5，0.6，0.6].

所以整个$\text{word embedding}$（广义）具体在做什么事情，可用两步来概括：

1. 对$\text{context}$进行分词操作。
2. 对分词进行$\text{one-hot}$编码，根据学习相应的权重对$\text{one-hot}$编码进行$\text{N(embedded\_dim)}$维空间的映射。

当对这$\text{N}$维空间的点降维处理后，会发现含义相近的词互相之间的距离会变得比较近。