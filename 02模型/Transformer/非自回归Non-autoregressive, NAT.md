**核心机制**： 一步预测全部目标词，输入为固定长度的空白序列（如占位符或源语言编码）。例如，NAT模型直接生成“你好世界”的所有词，无需逐步迭代。

![[Pasted image 20250315213251.png|300]]

需解决**目标词条件独立假设过强**的问题，常见方法包括：
-  **引入隐变量**：如Fertility模型预测源词到目标词的复制次数。
-  **迭代修正**：多轮并行生成，逐步优化结果（如CMLM、GLAT）。

**适用场景**：
- **实时性任务**：语音识别、实时翻译，需低延迟响应。
- **长序列生成**：视频生成、长文本摘要，避免AR逐帧/逐句耗时。
- **资源受限环境**：移动端部署，利用并行计算加速。

**未来趋势**：AR与NAT的界限逐渐模糊，通过隐变量建模、迭代修正等技术，NAT性能逼近AR，成为工业界部署的热门选择。