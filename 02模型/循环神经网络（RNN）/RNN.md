
> [!tip] Title
> 
RNN 常常可以被Self-Attention所替代。
# RNN（循环神经网络）详解 
RNN将状态在自身网络中循环传递，可以接受时间序列结构输入。
![[Pasted image 20250313152128.png|700]]
- 一对一：固定的输入到输出，**如图像分类**
- 一对多：固定的输入到序列输出，**如图像的文字描述**
- 多对一：序列输入到输出，**如情感分析，分类正面负面情绪**
- 多对多：序列输入到序列的输出，**如机器翻译,称之为编解码网络**
- 同步多对多：同步序列输入到同步输出，**如文本生成，视频每一帧的分类，也称之为序列生成**
## 1. RNN基本定义与核心原理 
**循环神经网络（Recurrent Neural Network, RNN）** 是一种专门处理序列数据的神经网络，通过引入循环连接实现时间维度的信息传递。其核心特性是隐层状态（hidden state）在不同时间步之间共享权重，形成动态记忆机制。下图是简化版本。
![[循环神经网络的结构.excalidraw|80%]]
### 数学公式推导 
#### 前向传播： 
- **隐层状态更新**：

$$h_t​=g(W_{hh}\ ​h_{t−1}​+W_{xh}​\ x_t​+b_h​)$$ ^aadbf5
- **通常简化为**：

$$h_t = \tanh(W \cdot [h_{t-1}, x_t] + b_h)$$ ^4210c7

（通过合并权重矩阵$W = [W_{hh}, W_{xh}]$简化计算）。 
其中：
    $W_{hh}$​：隐藏层到隐藏层的权重矩阵，是$N \times N$权重矩阵，连接$N$个隐藏层单元从时刻$t-1$到时刻$t$（用于传递历史信息），图中$W$
    $h_{t−1}$​：前一个时间步的隐藏状态，是$N \times 1$的形状
    $h_t$​：当前时间步的隐藏状态；
    $W_{xh}$​：输入到隐藏层的权重矩阵，是$N \times K$权重矩阵，连接$K$个输入单元到$N$个隐藏，图中$U$
    $x_t​$：当前时间步的输入，是$K \times 1$ 的形状
    $b_h$​：隐藏层的偏置项；
    $g$：非线性激活函数（如tanh或ReLU）。
- **输出计算**：
$$ y_t​=g(W_{hy}\ h_t​+c)$$
        $W_{hy}$：$L \times N$权重矩阵，连接N个隐藏层单元到L个输出层单元，图中$V$
         $g$：输出层激活函数，常用$Sigmoid$、$tanh$与整流线性单元。
        也即输入是$K \times 1$ 的形状，输出是$L \times 1$ 的形状。
    注意：**隐层状态更新**也可以受到上一个时刻的输出影响：$\mathbf{h}_t=f(\mathbf{W}_{hh}\mathbf{h}_{t-1}+\mathbf{W}_{xh}\mathbf{x}_t+\mathbf{W}_{yh}\mathbf{y}_{t-1})$
        $W_{yh}$：连接输出层到隐层的$N\times L$权重矩阵
![[RNN 完整流程.png|1000]]
##### 公式的逐项解释
1. **输入与历史信息的融合**
    公式中的两部分相加：
    - $W_{xh}​\ x_t​$：将当前输入$x_t$​通过权重矩阵$W_{xh}​$映射到隐藏层；
    - $W_{hh}\ h_{t−1}$：将前一步的隐藏状态$h_{t−1}$​通过权重矩阵$W_{hh}$​传递到当前步。 这种设计使得RNN能够同时结合当前输入和历史信息，捕捉序列中的时间依赖性。
2. **非线性激活函数的作用** 激活函数 $f$（如tanh）引入非线性，增强模型对复杂模式的表达能力。例如，tanh函数将输出限制在[-1, 1]之间，防止数值爆炸，同时保留梯度信息。
3. **隐藏状态的意义** htht​作为内部状态，承载了序列截止到当前时间步的所有有效信息。例如，在处理句子“I love machine learning”时，$h3$​不仅包含当前词“machine”的信息，还编码了前面“I love”的语义。
##### 公式的物理意义
- **时序记忆**：通过递归更新$h_t$​，RNN将历史信息逐层传递，形成动态记忆。例如，在时间步$t=4$，$h_4​$的生成依赖于$h_3$和输入“learning”，从而保留整个句子的上下文。
- **参数共享**：权重矩阵$W_{xh}$​和$W_{hh​}$在所有时间步共享，减少了模型参数，使RNN能处理任意长度的序列。
###### 示例说明
假设初始隐藏状态$h_0​=0$，输入序列为[0.5, 0.3, 0.8]，权重$W_{xh}​=0.7$、$W_{hh}​=0.6$，偏置$b_h​=0.2$，激活函数为tanh。则：
- 时间步1：$h_1=tanh(0.7×0.5+0.6×0+0.2)=tanh(0.55)≈0.5$
- 时间步2：$h_2=tanh(0.7×0.3+0.6×0.5+0.2)=tanh(0.71)≈0.61$通过逐步迭代，隐藏状态逐步累积序列信息

#### 局限性及改进
尽管上述公式是RNN的基础，但传统RNN存在**梯度消失/爆炸**问题，难以捕捉长距离依赖（如超过几十步的序列）。为此，LSTM和GRU等变体通过引入门控机制（如输入门、遗忘门）改进隐藏状态的计算，增强长期记忆能力

--- 
## 2. RNN与传统神经网络的对比及优势 
- ### 对比：

| **特性** | **传统神经网络** | **RNN**     |
| ------ | ---------- | ----------- |
| 输入结构   | 固定长度独立输入   | 可变长度序列输入    |
| 权重共享   | 无          | 时间步共享权重     |
| 历史信息利用 | 无法保留       | 通过隐状态保留时序信息 |
### **优势**： 
1. **动态序列处理**：适应语音、文本等时序数据的可变长度特性。 
2. **上下文建模**：通过隐状态捕捉长/短期依赖关系，适合语言模型、音频分析等任务。 
3. **参数效率**：权重共享减少模型复杂度，降低过拟合风险。 
--- 
## 3. RNN的特点与缺陷 
### **核心特点**： 
- **循环结构**：隐状态跨时间步传递，形成记忆链。 
- **权值共享**：同一权重矩阵处理所有时间步输入，增强泛化能力。
- **多输入输出模式**：支持序列到序列（如翻译）、单输入到序列（如生成音乐）等灵活结构。 
### **主要缺陷**：
1. **梯度问题**：长序列训练中梯度易消失/爆炸，限制长期依赖学习。 
2. **计算效率低**：时间步需顺序计算，难以并行化。 
3. **记忆容量限制**：基础RNN隐状态难以有效存储长期信息。 
4. **并行困难**：由于$h_t$取决于$h_{t-1}$，也即顺序依次的产生隐层特征，因此$RNN$很难并行计算。
5. **捕获长上下文**：$h_t$通常具有固定维度，所有历史信息都被浓缩为固定长度的载体，这使得也变得困难。
### **改进方向**： 
1. **门控机制**： 
- **LSTM**：引入输入门、遗忘门、输出门，控制信息流动。 $$ f_t = \sigma(W_f \cdot [a_{t-1}, x_t] + b_f) $$ （遗忘门决定保留多少历史信息）
- **GRU**：简化LSTM结构，合并门控单元减少参数。 
1. **双向结构**：Bidirectional RNN同时捕捉前后文信息。
2. **注意力机制**：增强对关键时间步的关注，缓解长程依赖问题。 
--- 
## 4. RNN在音频领域的应用 
### **应用场景**： 
1. **语音识别**： - 将音频波形转为文本，RNN建模声学特征的时间依赖性。 
2. **音乐生成**： - 使用sound-rnn等项目生成旋律，通过序列预测生成音符序列。 
3. **音频降噪**： - 结合LSTM预测噪声模式，实现实时背景噪声抑制。 
4. **语音合成（TTS）**： - 基于RNN的声码器（如WaveRNN）生成高质量语音波形。 
### **技术挑战**： 
- **实时性要求**：需优化计算效率以适应低延迟场景（如通话降噪）。 
- **多模态融合**：结合文本、图像信息生成情境化音频（如视频配音）。 
- **硬件适配**：在嵌入式设备中部署轻量化RNN模型（如MobileRNN）。