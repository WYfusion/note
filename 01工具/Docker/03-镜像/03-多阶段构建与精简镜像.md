# 多阶段构建与精简镜像

## 适用场景
- 镜像体积太大（深度学习镜像动辄 10GB+），传输慢、部署慢。
- 镜像里包含了源码、编译器（GCC/NVCC）、密钥等不该上线的东西。
- 希望提升安全性（减少攻击面）。

## 核心技术：多阶段构建 (Multi-stage Build)
在 Docker 17.05 之前，为了把编译产物和运行环境分开，通常需要两个 Dockerfile。
现在只需要一个 Dockerfile，使用多个 `FROM` 指令。

### 示例：Python 深度学习应用（分离编译依赖）
很多 Python 库（如 `numpy`, `pandas`）安装时可能需要编译（如果没有 wheel 包），或者你需要编译自定义的 CUDA 算子。编译需要 `gcc`, `g++`, `nvcc` 等工具，但运行时不需要。

```dockerfile
# --- 第一阶段：构建 (Builder) ---
# 使用带开发工具的镜像 (devel)
FROM python:3.9-slim AS builder

WORKDIR /app

# 安装编译依赖 (gcc, g++, make 等)
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# 创建虚拟环境
RUN python -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# 安装依赖 (有些包可能需要编译)
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# --- 第二阶段：运行 (Runtime) ---
# 使用精简镜像 (slim)，不带编译器
FROM python:3.9-slim

WORKDIR /app

# 只把第一阶段构建好的虚拟环境拷过来
COPY --from=builder /opt/venv /opt/venv

# 激活虚拟环境
ENV PATH="/opt/venv/bin:$PATH"

# 拷贝代码
COPY . .

CMD ["python", "app.py"]
```
**效果**：Runtime 镜像里没有 `gcc` 等编译工具，体积更小，也更安全。

## 基础镜像选择指南
选对基础镜像，体积优化就成功了一半。

### 1. `scratch` (0 MB)
- **特点**：真正的空镜像，啥都没有。
- **适用**：静态编译的 Go/Rust 程序。
- **缺点**：没有 shell，没法 `docker exec` 进去调试；没有 CA 证书。

### 2. `distroless` (~20 MB)
- **特点**：Google 出品，包含运行时（如 Python/Java）和 glibc，但**没有 Shell**。
- **适用**：追求极致安全的生产环境。
- **缺点**：调试困难。

### 3. `alpine` (~5 MB)
- **特点**：最流行的精简镜像，有 Shell，有包管理器 (`apk`)。
- **坑点**：使用 **musl libc** 而不是标准的 glibc。
  - **深度学习大坑**：PyTorch, TensorFlow 等通常只提供 glibc 的 wheel 包。在 Alpine 上安装它们需要从源码编译，极其痛苦且漫长。
  - **结论**：**搞深度学习/数据科学，千万别用 Alpine！**

### 4. `slim` 系列 (如 `python:3.9-slim`, `debian:bullseye-slim`)
- **特点**：删除了文档、手册等非必要文件，但保留了标准 glibc 和 apt。
- **适用**：**推荐作为 Python/深度学习的默认选择**。兼容性好，体积适中。

## 层优化技巧 (Layer Optimization)

### 1. 合并 RUN 指令
Docker 每一层都是独立的。如果你先 `COPY` 大文件，下一层 `RUN rm`，文件其实还在历史层里，体积不会减小。

**错误写法**：
```dockerfile
# 假设下载了一个 500MB 的数据集解压
RUN wget http://example.com/big-dataset.zip
RUN unzip big-dataset.zip
RUN rm big-dataset.zip
```
**结果**：镜像体积增加了 500MB（zip 包还在历史层里）。

**正确写法**（同一层里下载、解压、删除）：
```dockerfile
RUN wget http://example.com/big-dataset.zip \
    && unzip big-dataset.zip \
    && rm big-dataset.zip
```

### 2. 利用构建缓存
把变化最少的部分放在 Dockerfile 前面。

**优化前**（每次改源码，依赖都要重装）：
```dockerfile
COPY . .
RUN pip install -r requirements.txt
```

**优化后**（只改源码时，pip install 层直接用缓存）：
```dockerfile
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
```

### 3. 使用 .dockerignore
防止 `.git` (可能几百 MB)、`data/` (数据集)、`weights/` (模型权重) 被 COPY 进镜像。

