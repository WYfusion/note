上采样和下采样是深度学习中调整数据维度的核心技术，广泛应用于图像、语音、视频等领域。二者互为补充：**下采样**负责压缩维度、提取特征，**上采样**负责恢复维度、重建细节。

---

## 一、核心概念对比

| **特性**       | **下采样 (Downsampling)**    | **上采样 (Upsampling)**      |
| -------------- | ---------------------------- | ---------------------------- |
| **目标**       | 降低分辨率，压缩数据维度     | 提高分辨率，恢复细节信息     |
| **数学本质**   | 信息压缩（有损/无损）        | 信息扩展（需填补缺失值）     |
| **常见操作**   | 池化、跨步卷积               | 转置卷积、插值、反池化       |
| **网络位置**   | 编码器、特征提取层           | 解码器、输出重建层           |
| **维度变化**   | 空间↓ 通道↑                  | 空间↑ 通道↓                  |
| **信息保留**   | 部分丢失高频细节             | 依赖先验知识或跳跃连接补偿   |

---

## 二、下采样 (Downsampling)

### 2.1 核心方法

| **方法**                         | **特点**                             |
| -------------------------------- | ------------------------------------ |
| **最大池化 (Max Pooling)**       | 保留显著特征，对平移具有一定不变性   |
| **平均池化 (Avg Pooling)**       | 平滑特征，保留更多背景信息           |
| **跨步卷积 (Strided Conv)**      | 步长>1，可学习的下采样，参数化压缩   |
| **空间金字塔池化 (SPP)**         | 提取多尺度上下文信息                 |

### 2.2 输出尺寸计算

对于输入尺寸 $H_{in} \times W_{in}$，卷积核大小 $k$，步长 $s$，填充 $p$：

$$H_{out} = \left\lfloor \frac{H_{in} + 2p - k}{s} \right\rfloor + 1$$

**示例**：输入 $224 \times 224$，$k=3, s=2, p=1$ → 输出 $112 \times 112$

### 2.3 作用与意义

- **减少计算量**：降低后续层的参数量和计算复杂度
- **扩大感受野**：每个神经元覆盖更大的输入区域，捕捉全局语义
- **提取抽象特征**：通道数增加，学习更丰富的高层特征表示

> **本质**：压缩空间信息 → 扩展特征维度

---

## 三、上采样 (Upsampling)

### 3.1 核心方法

| **方法**                           | **特点**                                     |
| ---------------------------------- | -------------------------------------------- |
| **最近邻插值 (Nearest)**           | 简单快速，保边但有锯齿                       |
| **双线性插值 (Bilinear)**          | 平滑过渡，适合大多数场景                     |
| **转置卷积 (Transposed Conv)**     | 可学习参数，但易产生棋盘效应                 |
| **亚像素卷积 (Pixel Shuffle)**     | 通道→空间重排，高效且质量好                  |
| **反池化 (Unpooling)**             | 利用池化索引还原位置（如 SegNet）            |

### 3.2 转置卷积输出尺寸

$$H_{out} = (H_{in} - 1) \times s - 2p + k + \text{output\_padding}$$

**示例**：输入 $7 \times 7$，$k=4, s=2, p=1$ → 输出 $14 \times 14$

### 3.3 亚像素卷积原理（Pixel Shuffle）

将通道维度重排到空间维度：

$$X \in \mathbb{R}^{C \cdot r^2 \times H \times W} \to Y \in \mathbb{R}^{C \times rH \times rW}$$

其中 $r$ 为上采样因子。

### 3.4 PyTorch 示例

```python
# 最近邻上采样，放大2倍
m = nn.Upsample(scale_factor=2, mode='nearest')

# 双线性插值，指定输出尺寸
m = nn.Upsample(size=(16, 16), mode='bilinear', align_corners=True)

# 亚像素卷积（超分辨率常用）
m = nn.PixelShuffle(upscale_factor=2)

# 转置卷积
m = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=4, stride=2, padding=1)
```

### 3.5 作用与意义

- **恢复空间细节**：用于分割、检测等需要精确定位的任务
- **生成高分辨率输出**：超分辨率重建、图像生成（GAN）
- **语音合成**：从低维特征生成时域波形（如 WaveNet）

> **本质**：扩展空间信息 → 压缩特征维度

---

## 四、常见问题与优化策略

| **问题**               | **原因**                       | **解决方案**                                   |
| ---------------------- | ------------------------------ | ---------------------------------------------- |
| 小目标丢失             | 下采样过度压缩                 | 空洞卷积（Dilated Conv）、特征金字塔（FPN）    |
| 棋盘效应               | 转置卷积核重叠不均             | 插值 + 卷积组合、亚像素卷积                    |
| 细节模糊               | 上采样信息不足                 | 跳跃连接（Skip Connection）引入浅层特征       |

---

## 五、典型应用场景

| **任务**         | **下采样作用**         | **上采样作用**               |
| ---------------- | ---------------------- | ---------------------------- |
| 图像分类         | 提取全局语义特征       | —                            |
| 语义分割         | 编码上下文信息         | 恢复像素级分辨率             |
| 超分辨率         | —                      | 放大图像至高分辨率           |
| 图像生成（GAN）  | 判别器压缩特征         | 生成器逐步放大输出           |
| 语音处理         | 降低频谱时间分辨率     | 生成波形或频谱细节           |

