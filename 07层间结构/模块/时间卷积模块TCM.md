时间卷积模块（Temporal Convolutional Module, TCM）用于序列数据的局部时序特征提取。

---

## 一、核心思想

使用 **1D 卷积**提取局部时序特征，通过**膨胀卷积**扩大感受野，实现对长时依赖的建模。

---

## 二、基本结构

```
输入序列 X ∈ R^(B × C × T)
      │
      ├─ 1D Conv (kernel=k, dilation=d) → BN → ReLU
      │
      └─ 输出 Y ∈ R^(B × C' × T')
```

### 2.1 关键参数

| **参数**           | **典型值**       | **说明**                           |
| ------------------ | ---------------- | ---------------------------------- |
| kernel_size        | 3, 5, 7          | 卷积核大小                         |
| stride             | 1                | 步长（通常为 1 保留时序细节）      |
| dilation           | 1, 2, 4, 8...    | 膨胀率（指数增长）                 |
| padding            | causal           | 因果填充（保证不泄露未来信息）     |

---

## 三、感受野计算

### 3.1 单层感受野

对于卷积核大小 $k$，膨胀率 $d$：

$$RF_{single} = d \times (k - 1) + 1$$

**示例**：$k=3, d=4$ → $RF = 4 \times 2 + 1 = 9$

### 3.2 堆叠层的感受野

对于 $L$ 层卷积，每层膨胀率为 $d_l$，卷积核大小为 $k$：

$$RF_{total} = 1 + \sum_{l=1}^{L} d_l \times (k - 1)$$

**指数膨胀策略**（$d_l = 2^{l-1}$）：

$$RF_{total} = 1 + (k-1) \times (2^L - 1)$$

**示例**：$k=3$，4 层膨胀率 $\{1, 2, 4, 8\}$：
$$RF = 1 + 2 \times (1 + 2 + 4 + 8) = 1 + 2 \times 15 = 31$$

---

## 四、因果卷积（Causal Convolution）

确保输出仅依赖当前和过去的输入，不泄露未来信息。

### 4.1 因果填充量

$$\text{padding} = d \times (k - 1)$$

### 4.2 实现方式

```python
# 方式1：左填充
x = F.pad(x, (dilation * (kernel_size - 1), 0))
y = F.conv1d(x, weight, dilation=dilation)

# 方式2：使用 padding 参数 + 截断
y = F.conv1d(x, weight, padding=dilation * (kernel_size - 1), dilation=dilation)
y = y[:, :, :-padding]  # 截断右侧
```

---

## 五、压缩时间卷积模块（S-TCM） ^dc0bb6

S-TCM（Squeezed TCM）是 TCM 的轻量化变体，适用于实时场景。

### 5.1 压缩策略

| **策略**           | **方法**                                       |
| ------------------ | ---------------------------------------------- |
| **时间下采样**     | stride > 1 或池化，减少序列长度                |
| **固定膨胀组合**   | 使用 $\{1, 2, 4, 8\}$ 等固定膨胀率             |
| **自门控机制**     | 动态权重调节，避免冗余计算                     |

### 5.2 结构示意

```
输入 X
  │
  ├─ 1D Conv (stride=2) → BN → ReLU    ← 时间维度压缩
  ├─ Dilated Conv Block × N
  └─ 输出（时间维度减半）
```

---

## 六、时间卷积网络（TCN）

TCN 是由多个 TCM 堆叠形成的完整网络架构。

### 6.1 核心特点

| **特点**           | **说明**                                       |
| ------------------ | ---------------------------------------------- |
| **因果卷积**       | 保证时序因果性                                 |
| **膨胀卷积**       | 指数增长的膨胀率，感受野指数扩展               |
| **残差连接**       | 跨层信息传递，缓解梯度消失                     |
| **并行计算**       | 相比 RNN，卷积可高效并行                       |

### 6.2 TCN 残差块

```
输入 X
  │
  ├─ [主分支] Dilated Conv → BN → ReLU → Dropout
  │           Dilated Conv → BN → ReLU → Dropout
  │
  └─ [捷径分支] 1×1 Conv（若通道数不同）
      ↓
   逐元素相加 → 输出
```

---

## 七、与其他模块的结合

### 7.1 TCM + Transformer

TCM 作为预处理模块，先提取局部时序特征，再送入 Transformer 处理全局依赖：

```
输入序列
  │
  ├─ TCM（局部特征提取）
  ├─ Positional Encoding
  └─ Transformer Encoder（全局建模）
```

**优势**：
- TCM 压缩序列长度，降低 Transformer 的 $O(T^2)$ 复杂度
- 局部 + 全局的多尺度特征提取

---

## 八、应用场景

| **领域**       | **应用**                               |
| -------------- | -------------------------------------- |
| 语音处理       | 语音增强、语音识别、说话人识别         |
| 时间序列预测   | 股价预测、天气预报                     |
| 动作识别       | 视频动作分类                           |
| 音乐生成       | 自回归音频生成                         |
