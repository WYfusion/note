线性瓶颈结构（Linear Bottlenecks）由 **MobileNetV2**（2018, Sandler et al.）提出，是倒残差结构的关键设计。

---

## 一、核心思想

在倒残差结构的**最后一层 1×1 卷积后**，使用**线性激活（即不使用激活函数）**，而非 ReLU。

---

## 二、理论依据：流形假说与 ReLU 的信息损失

### 2.1 流形假说

深度神经网络中的特征可以被认为位于一个**低维流形（Manifold）** 上。

### 2.2 ReLU 在低维空间的问题

ReLU 函数定义为：
$$\text{ReLU}(x) = \max(0, x)$$

**关键问题**：ReLU 会将负值置零，导致**信息不可逆丢失**。

在**低维空间**中，这种信息丢失尤为严重：
- 设输入 $x \in \mathbb{R}^n$，经过线性变换 $Wx$ 后，若维度 $n$ 较小
- ReLU 可能将流形上的大量点映射到零，导致**流形坍塌**

### 2.3 数学分析

对于 ReLU 变换 $y = \text{ReLU}(Wx + b)$：

1. **高维空间**：即使部分维度被置零，仍有足够维度保留信息
2. **低维空间**：置零操作可能破坏流形结构，导致不可逆信息损失

**结论**：在低维瓶颈层使用 ReLU 会损害网络表达能力。

---

## 三、结构对比

| **位置**           | **残差结构（ResNet）** | **线性瓶颈（MobileNetV2）** |
| ------------------ | ---------------------- | --------------------------- |
| 升维后（高维空间） | ReLU                   | ReLU6                       |
| DW 卷积后          | ReLU                   | ReLU6                       |
| 降维后（低维空间） | ReLU                   | **Linear（无激活）**        |

---

## 四、结构示意

```
输入 x (低维，如24通道)
  │
  ├─ 1×1 Conv (升维→144) → BN → ReLU6    ← 高维，可用非线性
  ├─ 3×3 DW Conv → BN → ReLU6            ← 高维，可用非线性
  └─ 1×1 Conv (降维→24) → BN             ← 低维，使用线性激活
  │
  └─ [捷径分支] Identity
      ↓
   逐元素相加 → 输出
```

---

## 五、效果与意义

| **效果**               | **说明**                                       |
| ---------------------- | ---------------------------------------------- |
| **保留低维信息**       | 避免 ReLU 在低维空间的信息损失                 |
| **增强特征表达**       | 低维特征完整传递，提升模型性能                 |
| **与倒残差结构互补**   | 高维空间用非线性增强表达，低维空间用线性保留信息 |

---

## 六、实验验证

MobileNetV2 论文中的实验表明：
- 在瓶颈层使用线性激活比使用 ReLU 提升约 **2-3%** 的分类准确率
- 这验证了低维空间中 ReLU 会造成信息损失的理论分析
