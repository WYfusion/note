残差结构在**ResNet**提出

当模型深度增加时，模型的效果并不会变好（网络退化）

由于非线性激活函数ReLU的存在，导致输入输出不可逆，造成了模型的信息损失，更深层次的网络使用了更多的ReLU函数，导致了更多的信息损失，这使得浅层特征随着前项传播难以得到保存。

使用两个分支，一个捷径分支保留原输入的特征，一个主分支进行**降维、卷积、升维**

对于主分支：

- 通过1×1卷积核进行通道压缩减少通道数，降维操作
- 通过卷积核进行卷积处理
- 再次通过1×1卷积核进行通道扩展恢复原始通道数，升维操作

对于捷径分支：

- 一般不做处理，直接与主分支的输出进行相加操作，需要保障通道数、特征图尺寸大小均一致。
- 但是面对需要缩小特征图尺寸、加倍通道数时，也需要在此分支中添加合适卷积核

也就是说形成一个两头小中间大的瓶颈结构