残差结构（Residual Block）由 **ResNet**（2015, He et al.）提出。

---

## 一、背景：网络退化问题

当网络深度增加时，模型效果反而下降（网络退化，Degradation Problem）。这不是过拟合导致的，而是**优化困难**：
- 非线性激活函数（如 ReLU）导致信息不可逆传递
- 梯度在深层网络中难以有效回传
- 浅层特征难以保留到深层

---

## 二、核心思想：恒等映射

残差学习的核心是让网络学习**残差函数** $\mathcal{F}(x)$ 而非直接学习目标映射 $\mathcal{H}(x)$：

$$\mathcal{H}(x) = \mathcal{F}(x) + x$$

其中：
- $x$：输入（通过捷径分支直接传递）
- $\mathcal{F}(x)$：主分支学习的残差
- 若最优映射接近恒等映射，则 $\mathcal{F}(x) \to 0$ 比学习 $\mathcal{H}(x) \to x$ 更容易

---

## 三、结构类型

### 3.1 基础残差块（Basic Block）
适用于 ResNet-18/34 等浅层网络：

```
输入 x
  ├─ [主分支] 3×3 Conv → BN → ReLU → 3×3 Conv → BN
  │
  └─ [捷径分支] Identity（或 1×1 Conv 调整维度）
      ↓
   逐元素相加 → ReLU → 输出
```

### 3.2 瓶颈残差块（Bottleneck Block）
适用于 ResNet-50/101/152 等深层网络，形成**两头大、中间小**的瓶颈结构：

```
输入 x (256通道)
  ├─ [主分支] 1×1 Conv (降维→64) → BN → ReLU
  │           3×3 Conv (卷积→64) → BN → ReLU
  │           1×1 Conv (升维→256) → BN
  │
  └─ [捷径分支] Identity
      ↓
   逐元素相加 → ReLU → 输出
```

**设计原理**：
- **1×1 降维**：减少 3×3 卷积的计算量（如 256→64）
- **3×3 卷积**：提取空间特征
- **1×1 升维**：恢复通道数以匹配捷径分支

### 3.3 捷径分支处理

| **情况**               | **捷径分支操作**                          |
| ---------------------- | ----------------------------------------- |
| 维度匹配               | 恒等映射（Identity）                      |
| 空间尺寸下降（stride=2）| 1×1 Conv (stride=2) + BN                  |
| 通道数变化             | 1×1 Conv (调整通道) + BN                  |

---

## 四、计算量对比

设输入通道数为 $C$，瓶颈比例为 $r$（通常 $r=4$）：

| **结构**       | **参数量**                                       |
| -------------- | ------------------------------------------------ |
| Basic Block    | $2 \times (3^2 \times C \times C) = 18C^2$       |
| Bottleneck     | $1^2 \cdot C \cdot \frac{C}{r} + 3^2 \cdot \frac{C^2}{r^2} + 1^2 \cdot \frac{C}{r} \cdot C = \frac{11C^2}{r}$ |

当 $r=4$ 时，Bottleneck 参数量约为 Basic Block 的 **15%**。

---

## 五、1×1 卷积与线性变换的等价性

### 5.1 线性变换的本质

线性变换（Linear Transformation）实现 $y = Wx + b$，其中 $W \in \mathbb{R}^{N_{out} \times N_{in}}$。

### 5.2 1×1 卷积的等价性 ^37c3dd

对于输入 $X_{in} \in \mathbb{R}^{B \times C_{in} \times S}$（$B$: 批大小，$C_{in}$: 输入通道，$S$: 序列长度）：

- **1×1 卷积核**：$W \in \mathbb{R}^{C_{out} \times C_{in} \times 1}$
- **计算过程**：对每个空间/时间位置 $s$，执行 $y_s = W \cdot x_s + b$

由于卷积核大小为 1，**不涉及相邻位置的信息融合**，仅在通道维度进行线性变换，因此：

$$\text{Conv1D}(x, \text{kernel\_size}=1) \equiv \text{Linear}(x)$$

**优势**：
- 无缝融入 CNN 架构，保持张量形状一致性
- 权重在所有空间位置共享，参数高效

---

## 六、线性单元 vs 全连接层

| **特性**       | **线性单元（Linear Unit）**              | **全连接层（Fully Connected）**          |
| -------------- | ---------------------------------------- | ---------------------------------------- |
| **定义**       | 单个线性变换：$y = w^T x + b$            | 多个线性单元组合：$Y = WX + b$           |
| **输入/输出**  | 向量 → 标量                              | 向量 → 向量                              |
| **参数规模**   | $W \in \mathbb{R}^{1 \times N_{in}}$     | $W \in \mathbb{R}^{N_{out} \times N_{in}}$ |
| **应用场景**   | 逐元素线性变换                           | 特征映射、分类头                         |
