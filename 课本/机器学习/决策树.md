决策树是监督学习中一种基础且直观的非参数模型，因其结构类似 “树状” 决策流程而得名，广泛应用于分类和回归任务。下面从多个维度详细介绍这一模型：

### 一、决策树的核心概念与结构
#### 1. 定义
决策树通过对特征空间进行递归划分，生成一系列 “如果 - 那么”（if-then）规则，最终将样本映射到分类标签（分类树）或连续值（回归树）。它无需假设数据的分布形式，属于 “白盒模型”，可解释性强。
#### 2. 基本结构
- **根节点**：初始特征划分的起点，包含所有样本。
- **内部节点**：代表一个特征的判断条件，用于分裂样本集。
- **分支**：特征判断的结果（如 “温度> 25℃”），指向子节点。
- **叶节点**：最终决策结果，对应样本的类别或预测值。
#### 3. 直观例子
以 “是否去公园” 为例，决策树可能按以下逻辑构建：
```plaintext
根节点：天气是否晴朗？
├─ 是 → 内部节点：温度是否>25℃？
│  ├─ 是 → 叶节点：去公园（因天气好且温暖）
│  └─ 否 → 叶节点：去公园（天气好但不热）
└─ 否 → 叶节点：不去公园（天气不好）
```

### 二、决策树的工作原理：特征分裂与准则
决策树的核心是**如何选择最优特征进行分裂**，不同任务采用不同的分裂准则。
#### 1. 分类树的分裂准则
- **信息增益（ID3 算法）**
    - 基于信息熵（Entropy）衡量数据不确定性： $Ent(D) = -\sum_\limits{i=1}^k p_i \log_2 p_i$ 其中$p_i$是类别i在样本集D中的占比。
    - 按照离散属性at划分样本集合D后的综合信息熵$Ent(D,at)=\sum_i\frac{|D_i|}{|D|}Ent(D_i)$，其中的$D_i$是属性$at$取值为$at_i$的样本子集
    - 信息增益定义为分裂前后熵的减少： $Gain(D,at)=Ent(D)-Ent(D,at)$ 某属性在样本集D上的Gain值越大，使用此属性划分D获得的“纯度提升”越大
    - 信息增益最大的待选属性作为最优划分属性，$at_*=\max_{at\in A}Gain(D,at)$
    - **缺点**：倾向选择取值多的特征（如 ID 编号），因分裂后熵减少更明显。
    - 例子：
![[Pasted image 20250615165522.png|600]]

##### 1. 计算样本集 D 的信息熵 $\text{Ent}(D)$
- 样本总数 $|D| = 17$，其中 “好瓜 = 是”（8 个），“好瓜 = 否”（9 个）。$\text{Ent}(D) = -\frac{8}{17}\log_2\frac{8}{17} - \frac{9}{17}\log_2\frac{9}{17} \approx 0.998$
##### 2. 按 “色泽” 划分后的子节点熵
- **色泽 = 青绿**（6 个样本，3 是 3 否）：$\text{Ent}(D_{\text{青绿}}) = -\frac{3}{6}\log_2\frac{3}{6} - \frac{3}{6}\log_2\frac{3}{6} = 1$
- **色泽 = 乌黑**（6 个样本，4 是 2 否）：$\text{Ent}(D_{\text{乌黑}}) = -\frac{4}{6}\log_2\frac{4}{6} - \frac{2}{6}\log_2\frac{2}{6} \approx 0.918$
- **色泽 = 浅白**（5 个样本，1 是 4 否）：$\text{Ent}(D_{\text{浅白}}) = -\frac{1}{5}\log_2\frac{1}{5} - \frac{4}{5}\log_2\frac{4}{5} \approx 0.722$
##### 3. 综合信息熵 $\text{Ent}(D, \text{色泽})$
$\text{Ent}(D, \text{色泽}) = \frac{6}{17} \times 1 + \frac{6}{17} \times 0.917 + \frac{5}{17} \times 0.722 \approx 0.889$
##### 4. 信息增益 $\text{Gain}(D, \text{色泽})$
$\text{Gain}(D, \text{色泽}) = \text{Ent}(D) - \text{Ent}(D, \text{色泽}) \approx 0.998 - 0.889 = 0.109$
- **信息增益比（C4.5 算法）**
    - 对信息增益进行规范化，除以特征的 “固有值”（分裂前特征的熵）： $\text{信息增益比} = \frac{\text{信息增益}}{H_{\text{特征}}}$
    - 缓解了信息增益对多取值特征的偏好。
- **Gini 指数（CART 算法）**
    - 衡量样本集的不纯度，定义为： $\text{Gini}(D) = 1 - \sum_\limits{k=1}^n p_k^2$
    - 分裂后的 Gini 指数为子节点 Gini 的加权平均，选择使 Gini 指数减少最多的特征。
    - 对属性 at 的取值 $at_i$，划分为 $D_{\text{left}}$（取 $at_i$）和 $D_{\text{right}}$（不取 $at_i$），基尼指数为：$\text{Gini\_index}(D, at_i) = \frac{|D_{\text{left}}|}{|D|}\text{Gini}(D_{\text{left}}) + \frac{|D_{\text{right}}|}{|D|}\text{Gini}(D_{\text{right}})$
    - 遍历所有属性 $at \in A$ 及其取值 $at_i$，选择基尼指数最小的属性 - 取值组合：$(at_*, at_i^*) = \arg\min_{at\in A, at_i\in \text{取值}(at)} \text{Gini\_index}(D, at_i)$ （目标：最小化基尼指数，提升划分后的数据纯度，类比 ID3 的最大化信息增益，但准则不同。）
    - **基尼指数最小的待选属性**作为**最优划分属性**
    - **左子节点（取最优属性值$at_i$，如形状 = 圆）**：
    - 若子节点样本集纯（基尼值 = 0，如 3 个正样本），直接作为**叶节点**，赋值为类别（正），无需进一步分裂。
    - 若子节点非纯（基尼值 > 0，如假设分裂后仍有混合类别），则**递归对该子节点重复分裂流程**（重新计算剩余属性的基尼指数，选择最优划分）。
    - **右子节点（不取$at_i$，如形状 = 非圆）**：
    - 同样检查纯度：纯则为叶节点，非纯则**递归分裂**（如处理非圆样本时，选馅料的量作为新的最优属性，继续二叉分裂）。
![[Pasted image 20250620000444.png|600]]
### 基于信息增益ID3 算法构建决策树的详细过程
ID3 算法基于信息增益（Information Gain）选择划分特征，下面是对馅饼分类问题的完整推导。
###### 1. 计算根节点的信息熵
- 类别分布：正类 5 个，负类 3 个。
- 信息熵：$Entropy(S) = -\frac{5}{8}\log_2(\frac{5}{8}) - \frac{3}{8}\log_2(\frac{3}{8}) \approx 0.954$。
###### 2. 计算各特征的信息增益
特征：**形状**（圆 / 正方形 / 三角形）
- 子集 1（圆）：3 正 0 负，$Entropy(S_1) = 0$。
- 子集 2（正方形）：2 正 2 负，$Entropy(S_2) = -\frac{2}{4}\log_2(\frac{2}{4}) - \frac{2}{4}\log_2(\frac{2}{4}) = 1$。
- 子集 3（三角形）：0 正 1 负，$Entropy(S_3) = 0$。
- 加权平均熵：$\frac{3}{8} \times 0 + \frac{4}{8} \times 1 + \frac{1}{8} \times 0 = 0.5$。
- 信息增益：$Gain(形状) = 0.954 - 0.5 = 0.454$。
特征：**馅料的量**（大 / 小）
- 子集 1（大）：3 正 0 负，$Entropy(S_1) = 0$。
- 子集 2（小）：2 正 3 负，$Entropy(S_2) = -\frac{2}{5}\log_2(\frac{2}{5}) - \frac{3}{5}\log_2(\frac{3}{5}) \approx 0.971$。
- 加权平均熵：$\frac{3}{8} \times 0 + \frac{5}{8} \times 0.971 \approx 0.607$。
- 信息增益：$Gain(馅料的量) = 0.954 - 0.607 = 0.347$。
特征：**外壳大小**（大 / 小）
- 子集 1（大）：3 正 2 负，$Entropy(S_1) = -\frac{3}{5}\log_2(\frac{3}{5}) - \frac{2}{5}\log_2(\frac{2}{5}) \approx 0.971$。
- 子集 2（小）：2 正 1 负，$Entropy(S_2) = -\frac{2}{3}\log_2(\frac{2}{3}) - \frac{1}{3}\log_2(\frac{1}{3}) \approx 0.918$。
- 加权平均熵：$\frac{5}{8} \times 0.971 + \frac{3}{8} \times 0.918 \approx 0.950$。
- 信息增益：$Gain(外壳大小) = 0.954 - 0.950 = 0.004$。
###### 3. 选择根节点划分特征
- **最大信息增益**：形状（0.454）。
- 根节点划分：按形状将样本分为圆、正方形、三角形。
###### 4. 递归处理子节点
子节点 1（形状 = 圆）
- 样本：全为正类 → 叶子节点（类别：正）。
子节点 2（形状 = 三角形）
- 样本：全为负类 → 叶子节点（类别：负）。
子节点 3（形状 = 正方形）
- 剩余特征：**馅料的量**、**外壳大小**。
- 类别分布：2 正 2 负 → $Entropy(S) = 1$。
计算剩余特征的信息增益：
- 特征：馅料的量
    - 子集 1（大）：2 正 0 负，$Entropy(S_1) = 0$。
    - 子集 2（小）：0 正 2 负，$Entropy(S_2) = 0$。
    - 加权平均熵：0 → 信息增益 = 1 - 0 = 1。
- 特征：外壳大小
    - 子集 1（大）：1 正 1 负，$Entropy(S_1) = 1$。
    - 子集 2（小）：1 正 1 负，$Entropy(S_2) = 1$。
    - 加权平均熵：1 → 信息增益 = 1 - 1 = 0。
- 选择划分特征：**馅料的量**（信息增益 = 1）。
###### 5. 最终决策树结构
```plaintext
形状
├─ 圆 → 正（叶子）
├─ 三角形 → 负（叶子）
└─ 正方形
   ├─ 馅料的量大 → 正（叶子）
   └─ 馅料的量小 → 负（叶子）
```

### 基于 GINI 指数的 CART 决策树构建过程
###### 1. 初始根节点（所有 8 个样本）
- **类别分布**：正类 5 个（1,2,5,7,8），负类 3 个（3,4,6）。
- **GINI 指数**：$GINI_{root} = 1 - (5/8)^2 - (3/8)^2 = 30/64 = 0.46875$。
###### 2. 选择第一个划分特征（比较所有特征）
- 特征：**外壳大小**（大 / 小）
    - 大（5 样本，正 3 负 2）：$GINI_1 = 1 - (3/5)^2 - (2/5)^2 = 12/25 = 0.48$
    - 小（3 样本，正 2 负 1）：$GINI_2 = 1 - (2/3)^2 - (1/3)^2 = 4/9 ≈ 0.444$
    - 划分后 GINI：$(5/8)×0.48 + (3/8)×0.444 ≈ 0.4667$。
- 特征：**形状**（圆 / 正方形 / 三角形）
    - 圆（3 样本，全正）：$GINI_1 = 0$
    - 正方形（4 样本，正 2 负 2）：$GINI_2 = 1 - (2/4)^2 - (2/4)^2 = 0.5$
    - 三角形（1 样本，全负）：$GINI_3 = 0$
    - 划分后 GINI：$(3/8)×0 + (4/8)×0.5 + (1/8)×0 = 0.25$（**最小，选择此特征**）。
- 特征：**馅料的量**（大 / 小）
    - 大（3 样本，全正）：$GINI_1 = 0$
    - 小（5 样本，正 2 负 3）：$GINI_2 = 1 - (2/5)^2 - (3/5)^2 = 12/25 = 0.48$
    - 划分后 GINI：$(3/8)×0 + (5/8)×0.48 = 0.3$（大于形状的 0.25，不选）。
###### 3. 根节点划分（形状）
- 子节点 1：圆（3 样本，全正） → **叶子节点，类别：正**（GINI=0，无需划分）。
- 子节点 2：三角形（1 样本，全负） → **叶子节点，类别：负**（GINI=0，无需划分）。
- 子节点 3：正方形（4 样本，正 2 负 2） → 需继续划分。
###### 4. 划分正方形子节点（剩余特征：外壳大小、馅料的量）
- 特征：**外壳大小**（大 / 小）
    - 大（2 样本，正 1 负 1）：$GINI_1 = 0.5$
    - 小（2 样本，正 1 负 1）：$GINI_2 = 0.5$
    - 划分后 GINI：$0.5$（较高）。
- 特征：**馅料的量**（大 / 小）
    - 大（2 样本，全正）：$GINI_1 = 0$
    - 小（2 样本，全负）：$GINI_2 = 0$
    - 划分后 GINI：0（**最小，选择此特征**）。
###### 5. 正方形子节点划分（馅料的量）
- 子节点 1：大（2 样本，全正） → **叶子节点，类别：正**（GINI=0）。
- 子节点 2：小（2 样本，全负） → **叶子节点，类别：负**（GINI=0）。

###### 最终决策树结构

```plaintext
形状
├─ 圆 → 正（叶子）
├─ 三角形 → 负（叶子）
└─ 正方形
   ├─ 馅料的量大 → 正（叶子）
   └─ 馅料的量小 → 负（叶子）
```
###### 验证与解释
- **根节点划分形状**：将样本分为圆（全正）、三角形（全负）、正方形（需进一步划分），GINI 降至 0.25，有效分离纯类别。
- **正方形子节点划分馅料**：完美分离正（馅料大）和负（馅料小），GINI 为 0，确保子节点纯质。
- 所有样本均被正确分类，无冲突，符合 CART 树基于 GINI 指数的最优划分规则。
#### 2. **回归树的分裂准则**
- **方差减少**：目标是最小化分裂后子节点的样本方差，即： $\text{方差减少} = \text{父节点方差} - \sum_\limits{j=1}^m \frac{|S_j|}{|S|} \text{子节点}j\text{的方差}$ 选择使方差减少最大的特征与分裂点。
### 三、常见决策树算法与差异
#### 1. **ID3（Iterative Dichotomiser 3）**
- **特点**：使用信息增益作为分裂准则，仅支持离散特征，无法处理缺失值，容易过拟合。
- **局限**：未考虑特征取值数的影响，且不支持剪枝。
#### 2. **C4.5**
- **改进**：用信息增益比替代信息增益，支持连续特征（离散化处理）和缺失值，引入后剪枝（基于误差率）。
- **优势**：对噪声数据更鲁棒，应用更广泛。
#### 3. **CART（Classification and Regression Tree）**
- **特点**：支持分类和回归任务，使用 Gini 指数（分类）或方差减少（回归），采用二叉分裂（每个节点仅分裂为两个子节点）。
- **剪枝**：通过成本复杂度剪枝（Cost-Complexity Pruning），用验证集选择最优子树。
### 四、决策树的构建步骤
1. **初始化**：根节点包含所有训练样本。
2. **特征选择**：对每个特征，计算分裂后的准则增益（如信息增益），选择最优特征与分裂点。
3. **递归分裂**：按最优分裂条件生成子节点，对每个子节点重复步骤 2，直到满足停止条件（如样本数小于阈值、增益小于阈值、树深达到上限）。
4. **叶节点赋值**：分类树中，叶节点为样本多数类；回归树中，叶节点为样本均值。
### 五、过拟合与剪枝策略
决策树天然容易过拟合（因可能无限分裂至纯节点），需通过剪枝优化：
#### 1. 预剪枝（Pre-Pruning）
- 在构建过程中提前停止分裂，条件包括：
    - 树深不超过指定值；
    - 节点样本数小于阈值；
    - 分裂后的增益小于阈值。
- **优点**：计算效率高；**缺点**：可能过早停止，导致欠拟合。
#### 2. 后剪枝（Post-Pruning）
- 先构建完整树，再从叶节点向上删除 “贡献小” 的分支：
    - 用验证集评估剪枝前后的误差，若误差不增加则删除分支；
    - CART 的成本复杂度剪枝通过参数$\alpha$平衡树复杂度与误差。
- **优点**：效果更优，避免预剪枝的局限性；**缺点**：计算成本高。
### 六、决策树的优缺点
#### 优点
1. **可解释性强**：规则直观，适合向非技术人员解释决策逻辑。
2. **数据预处理要求低**：无需标准化特征，可自动处理特征间的非线性关系。
3. **适用范围广**：支持分类、回归，能处理离散和连续特征（需转换）。
4. **特征重要性评估**：可通过分裂次数或增益衡量特征重要性。
#### 缺点
1. **过拟合风险**：尤其在样本量小或特征多的场景中。增加限制条件控制模型复杂度、剪枝
2. **非稳定性**：数据微小变化可能导致树结构大幅改变（决策边界不光滑）。
3. **特征顺序敏感**：分裂顺序影响最终结构，可能忽略特征间的组合效应。
4. **处理高维稀疏数据差**：如文本数据，因多数特征取值稀疏，分裂增益低。