F1 分数是**查准率**（Precision，Pr）与**查全率**（Recall，Re）的**调和平均值**，公式为：$F_1 = 2 \cdot \frac{Pr \cdot Re}{Pr + Re}$
它平衡了模型对正类的 “精确性”（少误判）和 “完整性”（少漏判），只有两者都高时，F1 才会高（如 Pr=0.8、Re=0.8 时，F1=0.8；若 Pr=0.9、Re=0.7，F1≈0.79，略低）。
### 一、PR 曲线（Precision-Recall Curve）
#### 1. 定义与计算
- **精确率（Precision）**：预测为正类的样本中，实际为正类的比例，公式为 $Pr = \frac{TP}{TP + FP}$（TP：真阳性，FP：假阳性）。
- **召回率（Recall）**：实际为正类的样本中，被预测为正类的比例，公式为 $Re = \frac{TP}{TP + FN}$（FN：假阴性）。
- **PR 曲线**：以召回率（Recall）为横轴，精确率（Precision）为纵轴，通过调整分类阈值（如概率阈值从 0 到 1），绘制不同阈值下的 (P, R) 点并连接成曲线。每个点代表某一阈值下的精确率与召回率组合，反映模型对正类的预测能力。
#### 2. 应用场景与意义
- **正类稀少（如欺诈交易、疾病诊断）**：当正类占比极低（如 1%），ROC 曲线可能因负类样本多而 “虚高”（AUC 高但实际对正类预测差），PR 曲线更敏感。例如，医疗诊断中，需同时关注 “不漏诊（高召回）” 和 “不误诊（高精确）”，PR 曲线直观展示两者的权衡（如 Recall=0.95 时，Precision 是否可接受）。
- **假阴性（FN）代价高**：如癌症筛查，漏诊（FN）后果严重，需优先保证召回率（尽可能检测所有患者），PR 曲线的每个点直接体现这一需求（高召回下的精确率损失是否可接受）。
#### 3. 曲线特性
- **单调递减**：随着召回率增加（阈值降低，更多样本被预测为正），精确率通常下降（引入更多假阳性）。
- **平均精度（AP）**：曲线下面积，衡量模型对正类的综合性能。AP 越高，模型对正类的预测能力越强（类似 AUC，但针对正类不平衡场景更有效）。
![[Pasted image 20250615200750.png|600]]
### 二、ROC 曲线（Receiver Operating Characteristic Curve）
#### 1. 定义与计算
- **假阳性率（FPR）**：实际为负类的样本中，被预测为正类的比例，公式为 $FPR = \frac{FP}{TN + FP}$（TN：真阴性）。
- **真阳性率（TPR）**：公式为 $TPR = \frac{TP}{TP + FN}$。
- **ROC 曲线**：以 FPR 为横轴，TPR 为纵轴，调整阈值绘制的曲线。对角线（TPR=FPR）代表随机猜测，曲线越靠左上（远离对角线），模型性能越好。
#### 2. 应用场景与意义
- **类别平衡或关注整体性能**：当正负类分布较均衡（如 50:50），或需综合评估对正负类的预测能力（如垃圾邮件过滤，既减少漏判（FN，正常邮件归为垃圾）又减少误判（FP，垃圾邮件归为正常）），ROC 曲线更合适。其 AUC（Area Under Curve）值衡量模型区分正负类的能力，AUC=1 为完美模型，AUC=0.5 为随机模型。
- **鲁棒性评估**：ROC 曲线对类别不平衡的鲁棒性较强（FPR 和 TPR 均基于各自类别内的比例），适合比较不同模型的全局区分能力（如 AUC 对比）。
#### 3. 曲线特性
- **单调递增**：随着阈值降低（TPR 增加），FPR 也增加（更多负类被误判为正）。
- **AUC 解释**：随机选取一正一负样本，模型将正样本预测为正的概率高于负样本的概率，AUC 即为该概率（如 AUC=0.8，说明 80% 的情况下模型能正确区分正负样本）。
![[Pasted image 20250615200818.png|600]]
### 三、核心差异与选择依据

|**维度**|**PR 曲线**|**ROC 曲线**|
|---|---|---|
|**关注对象**|正类性能（精确与召回的权衡）|全局区分能力（TPR 与 FPR 的权衡）|
|**类别敏感性**|对正类稀少（<10%）或 FN 代价高敏感|对类别平衡或整体误判（FP+FN）敏感|
|**AUC/AP**|AP（正类性能，受正类比例影响大）|AUC（全局区分，对类别不平衡鲁棒）|
|**典型场景**|欺诈检测、医疗诊断（正类少，关注漏检）|垃圾邮件过滤、模型对比（类别平衡，关注整体误判）|

### 四、阈值选择与数据泄露
- **阈值选择**：必须在验证集（而非训练集）上调整阈值（如通过 PR 或 ROC 曲线找到最优阈值），避免 “数据泄露”（训练集信息污染阈值选择，高估性能）。例如，在训练集上选阈值使 Precision=1，验证时必然下降，因阈值过拟合训练集。
- **经验法则**：
    - **正类稀少（<10%）或 FN 代价高（如漏检严重）**：优先使用 PR 曲线，直观反映正类的精确与召回权衡。
    - **类别平衡（如≥20% 正类）或关注整体误判（FP+FN）**：使用 ROC 曲线，其 AUC 更全面衡量模型区分能力。
### 五、实例说明
- **PR 曲线实例（欺诈检测，正类 1%）**： 模型在 Recall=0.8（80% 欺诈被检测）时，Precision=0.2（20% 检测为欺诈的是真欺诈），说明大量正常交易被误判（FP 高）。PR 曲线清晰展示这一 trade-off，帮助决策是否接受（如业务可容忍误判，优先召回）。
- **ROC 曲线实例（垃圾邮件，50% 正类）**： AUC=0.9，FPR=0.1（10% 正常邮件误判为垃圾），TPR=0.9（90% 垃圾邮件正确识别），整体性能优。ROC 曲线简洁反映全局区分能力，适合模型对比。

### R²（决定系数）
#### 1. 定义与公式
- **决定系数（R²）** 衡量回归模型预测能力的度量指标，公式为：$R^2 = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}$ 其中，分子是**MSE 均方误差**，分母是**总平方和（TSS，均值预测误差）**。$R²$ 越大，模型拟合优度越高（越接近 1）。
#### 2. 直观意义
- **解释变异比例**：$R^2=0.8$ 表示模型解释了 80% 的因变量波动，剩余 20% 由噪声或未建模因素解释。
- **与基准对比**：以 “均值预测” 为基准（最差模型，$R^2=0$），$R²>0$ 说明模型优于随机猜测，$R²<0$ 则不如均值预测（模型无效）。
­$R^2=1$  预测值与真实值完全相同
­$0<R^2<1$ 度量拟合程度的好坏
­$R^2=0$ 预测性能等同于均值这个baseline

### 泛化误差来源
总误差=可约误差+不可约误差(噪声)
偏差: 度量学习算法的**期望预测与真实结果的偏离程度**  ­期望输出与真实结果的差距，­偏差越大，数据距离真实越远
方差:度量学习算法在**不同训练集上模型​​的预测波动性**  ­输出值与期望输出间的差距，­模型输出稳定性，­方差越大，数据分布越分散
定义在学习算法而非单个模型上
**欠拟合——高偏差**
**过拟合——高方差**

**减少偏差**
- ­使用更复杂的模型
- ­增加特征数量
- ­增加训练数据集
**减少方差**
- ­正则化
- ­特征选择
- ­集成学习
- 交叉验证
### 学习曲线用于诊断模型性能
- 欠拟合：训练与测试指标均较差且差距小
- 过拟合：训练与测试指标存在一定差距
- 良好拟合：训练与测试指标趋于稳定，且差距小
学习曲线也可以用于**调整模型超参数**