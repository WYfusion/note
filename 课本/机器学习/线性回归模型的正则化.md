![[Pasted image 20250615161008.png|600]]
![[Pasted image 20250615161045.png|600]]
正常训练得到的是经验风险的损失函数，添加一个模型复杂度的正则项或者惩罚项作为矫正
![[Pasted image 20250615161249.png|600]]
### 岭回归
MSE尽量小，实现拟合训练数据
正则项取系数的平方和（L2范数）控制模型复杂度
$$L=\sum_{i=1}^m(\boldsymbol{\theta}^T\boldsymbol{x}_i+b-y_i)^2+\alpha\frac{1}{2}\sum_{k=1}^n\theta_k^2\quad(\alpha>0)$$
具体的解受$α$取值的影响
闭式解
$$\widehat{\boldsymbol{\theta}}=(X^TX+\alpha I)^{-1}X^T\boldsymbol{Y}$$

梯度下降
$$\frac{\partial}{\partial\theta_j}MSE(\theta)=\frac{2}{m}\sum_{i=1}^m(\hat{y}^{(i)}-y^{(i)})x_j^{(i)}+\alpha\theta_j$$
$\widehat{\boldsymbol{\theta}}$系数的所有元素都接近于0
某个特征对应的系数越小，对输出的影响越小
所有特征的系数越小，模型越简单
### 注意
**岭回归对属性尺度非常敏感**，执行前必须对**数据**进行**缩放**
参数$α$控制模型正则化的程度

### 套索回归
正则项取系数的绝对值之和（L1范数）
代价函数
$$L=\sum_{i=1}^m(\boldsymbol{\theta}^T\boldsymbol{x}_i+b-y_i)^2+\alpha\sum_{k=1}^n|\theta_k|\mathrm{(α>0)}$$
L不是连续可导的，无闭式解，也无法直接应用梯度下降，需要用次梯度下降法、坐标下降法
$\widehat{\boldsymbol{\theta}}$中很多为0的分量
系数分量为0的属性可视作冗余
冗余属性越多，影响回归值的因素越少，模型越简单
可作为自动化的特征选择手段，增加模型的可解释性

### 线性回归模型特点
#### 优点
建模迅速，对于小数据量、简单的关系很有效
是许多强大的非线性模型的基础
线性回归模型十分容易理解，结果具有很好的可解释性，有利于决策分析
#### 缺点
对于非线性数据，或数据特征间具有相关性的多项式回归难以建模
难以很好地表达高度复杂的数据