![[Pasted image 20250609211738.png]]
![[Pasted image 20250609212455.png]]
#### 1. Grippo 准则（非单调 Armijo 型）
- **定义**： 对于下降方向 $d^k$（$\nabla f(x^k)^T d^k < 0$），步长 $\alpha$ 需满足：$f(x^{k+1}) \leq \max_\limits{0 \leq j \leq \min\{k, M\}} f(x^{k-j}) + c_1 \alpha \nabla f(x^k)^T d^k, \quad c_1 \in (0,1)$
    - **核心**：不要求 $f(x^{k+1}) < f(x^k)$，只需**不超过前 M 步的最大值**。例如，若前 $M=2$ 步的函数值为 $10, 12, 8$，当前步允许 $f(x^{k+1}) \leq 12 + \text{下降项}$（即使 $f(x^k)=8$，允许 $f(x^{k+1})$ 上升到 12 以下，只要下降项足够）。
    - **与 Armijo 对比**：Armijo 是 $M=0$ 的特例（仅对比当前步），Grippo 更宽容，适合非凸函数（如函数值短期上升但长期下降）。
#### 2. Zhang-Hager 准则（动态参考值）
- **定义**： 步长 $\alpha$ 满足：$f(x^{k+1}) \leq C^k + c_1 \alpha \nabla f(x^k)^T d^k,$ 其中 $C^k$ 递推为：$C^0 = f(x^0), \quad C^{k+1} = \frac{\eta Q^k C^k + f(x^{k+1})}{Q^{k+1}}, \quad Q^{k+1} = \eta Q^k + 1 \quad (\eta, c_1 \in (0,1))$
    - **核心**：$C^k$ 是历史函数值的加权平均（$\eta$ 控制权重，$\eta=0$ 时退化为 Armijo，$\eta>0$ 时包含历史信息）。例如，$\eta=0.5$ 时，$C^{k+1}$ 是 $C^k$（历史平均）和 $f(x^{k+1})$ 的平均，允许当前值在历史平均附近波动。
#### 3. 推导与收敛性（以 Grippo 为例）
- **下降项分析**：因 $\nabla f(x^k)^T d^k < 0$，设为 $-\beta_k$，则下降项 $-c_1 \alpha \beta_k \leq 0$。当 $\alpha \to 0$，$f(x^{k+1}) \approx f(x^k) - c_1 \alpha \beta_k \leq \max_{j \leq M} f(x^{k-j}) - c_1 \alpha \beta_k$（因 $f(x^k) \leq \max_\limits{j \leq M} f(x^{k-j})$ 对 $k \geq M$ 成立），故小步长必满足准则，算法终止。
- **全局收敛**：通过证明函数值序列的聚点满足 $\nabla f(x^*) = 0$（利用下降方向的累积效应，即使中间有上升，长期仍下降）。
#### 4. 通俗理解
- **Grippo**：“允许这次成绩没超过过去 M 次的最高分，只要有进步（下降项）”。比如训练模型，损失偶尔上升（如跳出局部极小），但不超过最近 M 次的最大损失，就继续。
- **Zhang-Hager**：“参考值是历史损失的平均，这次损失不超过平均加进步”。历史越久（$\eta$ 大），参考值越稳定，允许当前损失在平均附近调整，更灵活应对震荡。
#### 5. 应用与注意
- **参数**：M（Grippo）或 $\eta$（Zhang-Hager）控制非单调性，$c_1$ 类似 Armijo 的松弛因子。
- **场景**：非凸优化（如深度学习），避免单调搜索陷入局部极小，通过允许短期波动探索更优解。
### 总结
- **非单调准则**放宽了 Armijo 的严格下降要求，通过参考历史函数值（最大值或加权平均），增强对非凸函数的适应性。
- 推导中利用下降方向的负定性和小步长的泰勒展开，保证算法收敛，是解决复杂优化问题的有效工具。