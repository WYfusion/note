#### 1. 标量对向量求导
* **常数对向量求导**：
$$
\frac{\partial a}{\partial x} = 0
$$
其中，$a$ 是与向量 $x$ 无关的常数。
* **线性项对向量求导**：
$$
\frac{\partial x^T a}{\partial x} = \frac{\partial a^T x}{\partial x} = a
$$
其中，$a$ 是与向量 $x$ 维度相同的常数向量。
* **二次型对向量求导**：
$$
\frac{\partial x^T x}{\partial x} = 2x
$$
$$
\frac{\partial x^T A x}{\partial x} = (A + A^T)x
$$
若矩阵 $A$ 为对称阵，即 $A = A^T$或者正定阵，则上式简化为：
$$
\frac{\partial x^T A x}{\partial x} = 2Ax
$$
#### 2. 向量对向量求导
* **线性变换对向量求导**：
$$
\frac{\partial A x}{\partial x} = A^T
$$

其中，$A$ 是矩阵。
* **向量内积对向量求导（链式法则）**：
$$
\frac{\partial u^T v}{\partial x} = \frac{\partial u^T}{\partial x}v + \frac{\partial v^T}{\partial x}u
$$
其中，$u$ 和 $v$ 是关于向量 $x$ 的函数向量。
#### 3. 标量对矩阵求导
* **常数对矩阵求导**：
$$
\frac{\partial a}{\partial X} = 0
$$
其中，$a$ 是与矩阵 $X$ 无关的常数。
* **矩阵迹对矩阵求导**：
$$
\frac{\partial tr(A^T X)}{\partial X} = A
$$
其中，$tr(\cdot)$ 表示矩阵的迹。
* **二次型对矩阵求导**：
$$
\frac{\partial x^T A x}{\partial X} = x x^T \circ (A + A^T)
$$
其中，$\circ$ 表示Hadamard积（元素乘积）。注意，此公式可能因布局约定（分子布局或分母布局）而有所不同。
#### 4. 其他重要公式
* **线性回归中的损失函数求导**：
$$
\frac{\partial (X u - v)^T (X u - v)}{\partial X} = 2(X u - v) u^T
$$
其中，$u$ 和 $v$ 是向量，$X$ 是矩阵。
* **二次型扩展求导**：
$$
\frac{\partial u^T X^T X u}{\partial X} = 2 X u u^T
$$
其中，$u$ 是向量，$X$ 是矩阵。
这些公式在优化理论、机器学习、统计学等领域中有着广泛的应用，特别是在处理多变量函数的梯度、雅可比矩阵和海森矩阵时。熟练掌握这些公式可以显著提高复杂模型的推导效率。



