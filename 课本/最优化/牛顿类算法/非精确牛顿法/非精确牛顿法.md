![[Pasted image 20250610133042.png|800]]
### 背景：标准牛顿法的局限性
在无约束优化问题中，标准牛顿法迭代公式如下：
$$\nabla^2 f(x^k) d^k = -\nabla f(x^k)$$
即每一步都要求解一个线性方程组，其系数矩阵是 $Hessian \nabla^2 f(x^k)$。
但当变量维数 $n$ 很大时，面临两大问题：
- **计算与存储困难**：
    - 计算 Hessian 本身开销巨大（复杂度约 $O(n^2)$ 或更高）；
    - 存储 $n \times n$ 的 Hessian 矩阵开销大；
- **求解方程代价高**：
    - 对 Hessian 做逆或者 Cholesky 分解成本很高（复杂度约 $O(n^3)$）。
因此，引入了**非精确牛顿法（Inexact Newton Method）**，以提升效率。
### 非精确牛顿法的核心思想
核心思想：
不再精确求解牛顿方向，而是允许**在一定精度范围内近似求解牛顿方程**。
具体做法是：
- 用迭代法（如共轭梯度法 Conjugate Gradient）来求解近似线性系统；
- 在残差（residual）满足一定**容忍度条件**下提前停止，以降低计算代价。
### 数学表达与推导
#### 1. 定义残差向量
将非精确求解过程引入残差 $r^k$，表示“误差”或“不精确度”，改写牛顿方程为：
$$\nabla^2 f(x^k) d^k = -\nabla f(x^k) + r^k \tag{8}$$
这意味着我们不要求左边精确等于右边，只要误差 $r^k$ 控制在允许范围即可。
#### 2. 终止条件
为了确保最终方向仍然“有用”，需要对残差 $r^k$ 设置终止容限。常用准则如下：
$$\|r^k\| \leq \eta_k \|\nabla f(x^k)\| \tag{9}$$
其中：
- $\eta_k \in [0,1)$ 是控制精度的参数，称为**容忍系数**；
- 越小的 $\eta_k$表示要求解得越精确；
- 如果 $\eta_k = 0$，则回到标准牛顿法（精确求解）。
该条件的含义是：当前的非精确解 $d^k$ 所导致的残差 $r^k$ 的大小，不能超过当前梯度范数的一定比例。
### 理论分析（证明非精确牛顿法仍可收敛）
我们简要说明其收敛原理：
设目标函数 $f$ 是强凸、Hessian 为Lipschitz连续，则可证明：
- 如果 $\eta_k \to 0$，并且满足一定下降准则（如Armijo线搜索），则：
    - 非精确牛顿法的迭代仍能收敛到最优解；
    - 若 $\eta_k \to 0$ 足够快，则可保持超线性收敛（类似精确牛顿法）。