![[Pasted image 20250610133205.png|800]]
#### 逻辑回归模型
![[Pasted image 20250610133228.png|800]]
![[Pasted image 20250610133307.png|800]]
![[Pasted image 20250610133320.png|800]]
![[Pasted image 20250610133327.png|800]]
![[Pasted image 20250610133335.png|800]]

线搜索 Newton-CG 法是一种用于求解无约束优化问题的高效算法，结合了牛顿法的二次收敛性和共轭梯度法（CG）的 Hessian-free 特性，适用于大规模优化问题。以下是对该算法的详细阐述：
### 1. 算法背景与动机
- **牛顿法的瓶颈**：传统牛顿法需求解 $B_k d = -\nabla f(x^k)$（$B_k$ 为海瑟矩阵），但大规模问题中 $B_k$ 的存储和计算成本极高。
- **CG 法的优势**：CG 法求解线性方程组时，只需计算矩阵 - 向量积（无需显式存储矩阵），可设计为 **Hessian-free 方法**，避免海瑟矩阵的存储 / 计算难题。
- **负曲率检查**：CG 法要求系数矩阵正定，而 $B_k$ 可能非正定，需通过 $p_j^T B_k p_j \leq 0$ 检测负曲率方向，确保算法有效性。
### 2. 算法步骤（Algorithm 4）
#### 外层循环（牛顿迭代）
1. **初始化**：给定初始点 $x^0$。
2. **迭代更新**：对 $k=0,1,2,\ldots$，执行：
    - **内层 CG 迭代（求解牛顿方程 $B_k d = -\nabla f(x^k)$）**：
        - **参数初始化**（第 3 步）：$\epsilon_k = \min(0.5, \sqrt{\|\nabla f(x^k)\|})$（自适应终止阈值），$z_0=0$（初始近似解），$r_0=\nabla f(x^k)$（初始残差，对应方程 $B_k d = -r_0$），$p_0=-r_0$（初始搜索方向，最速下降方向）。
        - **CG 迭代（第 4-13 步）**：
            - **负曲率检查（第 5-7 步）**：若 $p_j^T B_k p_j \leq 0$（海瑟矩阵不正定，存在负曲率方向），终止 CG：
                - $j=0$ 时，返回 $d^k = -\nabla f$（最速下降方向，因初始搜索方向即负梯度）；
                - $j>0$ 时，返回 $d^k = z_j$（当前近似解，避免在非正定矩阵上无效迭代）。
            - **迭代更新（第 8 步）**：计算步长 $\alpha_j = \frac{r_j^T r_j}{p_j^T B_k p_j}$，更新近似解 $z_{j+1} = z_j + \alpha_j p_j$ 和残差 $r_{j+1} = r_j + \alpha_j B_k p_j$（CG 法的残差更新，对应 $B_k z_{j+1} + r_{j+1} = -r_0$，即 $B_k z_{j+1} \approx -r_0$ 当 $r_{j+1} \to 0$）。
            - **收敛检查（第 9-11 步）**：若 $\|r_j\| < \epsilon_k$，终止 CG，返回 $d^k = z_{j+1}$（近似牛顿方向）。
            - **共轭方向更新（第 12 步）**：计算 $\beta_{j+1} = \frac{r_{j+1}^T r_{j+1}}{r_j^T r_j}$，更新搜索方向 $d_{j+1} = -r_{j+1} + \beta_{j+1} d_j$（CG 法的共轭方向更新，确保方向共轭性）。
    - **线搜索与迭代更新（第 14 步）**：通过线搜索确定步长 $\alpha_k$（满足 Armijo 等条件，正定情形下取 $\alpha_k=1$ 即纯牛顿步），更新 $x^{k+1} = x^k + \alpha_k d^k$。
### 3. 关键特性与优势
- **Hessian-free**：仅需计算 $B_k p_j$（海瑟 - 向量积），无需显式存储 $B_k$，适用于大规模问题（如深度学习优化，利用反向传播高效计算海瑟 - 向量积）。
- **自适应终止**：$\epsilon_k$ 随梯度范数调整，平衡迭代精度与效率（梯度大时阈值宽松，梯度小时严格）。
- **处理非正定海瑟矩阵**：通过负曲率检查（第 5 步），避免 CG 法在非正定矩阵上的无效迭代，确保算法鲁棒性（返回近似方向或最速下降方向，保证下降性）。
- **线搜索增强收敛性**：即使海瑟矩阵不正定或函数非二次，线搜索（如 Armijo 规则）确保目标函数值下降，保证全局收敛性。
### 4. 应用场景
- **大规模优化**：如神经网络训练（损失函数的海瑟矩阵维度极高，Hessian-free 方法可避免存储）、大规模非线性规划等。
- **非二次函数优化**：通过线搜索调整步长，适应非二次目标函数，结合牛顿法的局部快速收敛性。
### 5. 算法分析
- **内层 CG 迭代**：理论上，若 $B_k$ 正定，CG 法在 n 步内（n 为变量维度）收敛到精确牛顿方向；若 $B_k$ 非正定，负曲率检查提前终止，返回近似方向，保证算法不会因矩阵非正定而崩溃。
- **外层收敛性**：线搜索确保每步目标函数下降，结合牛顿法的局部二次收敛性（当 $B_k$ 正定且步长为 1 时），整体算法具有超线性收敛速度。
### 总结
线搜索 Newton-CG 法通过 CG 法迭代求解牛顿方程（Hessian-free），嵌入负曲率检查和线搜索，高效处理大规模、非二次优化问题，是牛顿法在实际应用中的重要扩展，尤其适用于海瑟矩阵难以 存储/计算 的场景（如深度学习）。其核心思想是利用 CG 法的迭代特性，避免显式矩阵操作，同时通过自适应策略和线搜索保证算法的有效性与收敛性。